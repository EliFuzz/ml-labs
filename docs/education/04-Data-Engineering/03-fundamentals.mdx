---
title: Fundamentals
description: Fundamental concepts in data engineering, including databases, data storage solutions, and data processing techniques.
hide_table_of_contents: true
---

import TabItem from "@theme/TabItem";
import Tabs from "@theme/Tabs";

<Tabs queryString="primary">
  <TabItem value="de-lifecycle" label="DE Lifecycle">
    DE Lifecycle describes the stages involved in taking raw data from its origin to a usable format for analytics, reporting, and machine learning. The typical stages are **Generation**, where data is created; **Storage**, where it's held; **Ingestion**, where it's brought into a system; **Transformation**, where it's cleaned and processed; and **Serving**, where it's made available to users and applications. This structured process ensures the consistent delivery of high-quality data products and helps data engineers build reliable data pipelines.

    ```mermaid
    flowchart LR
      subgraph Generation
        direction LR

        A[Data Sources] e1@-->|APIs| B[Applications]
        A e2@-->|Sensors| C[IoT Devices]
        A e3@-->|Queries| D[Databases]
        A e4@-->|Files| E[File Systems]
        A e5@-->|Web Scraping| F[Web Services]
      end

      B e6@--> DP
      C e7@--> DP
      D e8@--> DP
      E e9@--> DP
      F e10@--> DP

      subgraph DP ["Data Platform"]
        direction LR
        subgraph " "
          direction LR
          ingest["Ingestion"]:::ingest e11@--> transform["Transformation"]:::transform e12@--> serve["Serving"]:::serve
        end
        storage["Storage"]:::storage
        ingest e13@--> storage
        transform e14@--> storage
        serve e15@--> storage
      end

      DP e16@--> analytics["Analytics"]:::output
      DP e17@--> ml["Machine Learning"]:::output
      DP e18@--> rETL["Reverse ETL"]:::output

      subgraph Undercurrents
        direction TB
        sec["Security"]:::uc_sec
        dm["Data management"]:::uc_dm
        dops["DataOps"]:::uc_dops
        darch["Data architecture"]:::uc_darch
        orch["Orchestration"]:::uc_orch
        se["Software engineering"]:::uc_se
      end

      classDef gen fill:#f7c6c6,stroke:#333,stroke-width:1px;
      classDef ingest fill:#1fbf90,stroke:#0f5a3f,stroke-width:1px,color:#fff;
      classDef transform fill:#b56ce6,stroke:#5a2e6a,stroke-width:1px,color:#fff;
      classDef serve fill:#2f86d6,stroke:#113f6c,stroke-width:1px,color:#fff;
      classDef storage fill:#d9d9d9,stroke:#8c8c8c,stroke-width:1px;
      classDef output fill:#f6df6b,stroke:#8f7a2a,stroke-width:1px;
      classDef uc_sec fill:#e7c8f4,stroke:#9b6aa5,stroke-width:1px;
      classDef uc_dm fill:#d8eef8,stroke:#6fa6bf,stroke-width:1px;
      classDef uc_dops fill:#f6f1d8,stroke:#a58f56,stroke-width:1px;
      classDef uc_darch fill:#c8efd8,stroke:#4f9a6e,stroke-width:1px;
      classDef uc_orch fill:#f7d0d0,stroke:#9b5a5a,stroke-width:1px;
      classDef uc_se fill:#f5ead4,stroke:#9b8f6a,stroke-width:1px;

      e1@{ animate: true }
      e2@{ animate: true }
      e3@{ animate: true }
      e4@{ animate: true }
      e5@{ animate: true }
      e6@{ animate: true }
      e7@{ animate: true }
      e8@{ animate: true }
      e9@{ animate: true }
      e10@{ animate: true }
      e11@{ animate: true }
      e12@{ animate: true }
      e13@{ animate: true }
      e14@{ animate: true }
      e15@{ animate: true }
      e16@{ animate: true }
      e17@{ animate: true }
      e18@{ animate: true }
    ```

    ## Stages

    <table className="text_vertical">
      <thead>
        <tr>
          <th>Stage</th>
          <th>Description</th>
          <th>Key Activities</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><b>Generation</b></td>
          <td>Data originates from source systems like databases, apps, IoT devices, APIs, files, and web services. Data engineers must understand their formats, generation velocity, and integration protocols</td>
          <td>Understanding data formats, generation velocity, integration protocols, schema analysis, connectivity, and business logic</td>
        </tr>
        <tr>
          <td><b>Evaluating Source Systems</b></td>
          <td>Data engineer must understand how source systems generate data, including their quirks, behaviors, and limitations to design effective ingestion pipelines</td>
          <td>Managing schemas, handling inconsistencies, and ensuring reliable data extraction</td>
        </tr>
        <tr>
          <td><b>Ingestion</b></td>
          <td>Ingestion refers to the process of moving data from generating sources into a centralized processing system (data lake, warehouse, stream processor), either in batch or real-time (streaming) modes. Source systems and ingestion are critical chokepoints - a single data hiccup can disrupt the entire pipeline, breaking downstream processes and creating ripple effects</td>
          <td>Selecting ingestion patterns (batch vs. streaming), validating and monitoring pipeline flows, handling schema drift, initial data quality checks</td>
        </tr>
        <tr>
          <td><b>Data Storage</b></td>
          <td>Data at every stage - raw, cleaned, modeled - may be persistently stored for reliability, auditability, and downstream processing. Storage architectures include data lakes (raw staging), data warehouses (structured, analytics-ready), and hybrid lakehouse solutions</td>
          <td>Choosing storage types, optimizing for scalability and cost, enforcing security and backup protocols, supporting data versioning and lineage</td>
        </tr>
        <tr>
          <td><b>Transformation</b></td>
          <td>Converts raw ingested data into cleaned, standardized, enriched formats suitable for analytics and ML. Transformations can be orchestrated via ETL/ELT tools, SQL scripts, or data workflow managers. The Medallion Architecture often structures this into Bronze (raw), Silver (cleaned), and Gold (aggregated) layers</td>
          <td>Cleansing, data normalization and format conversion, business logic and enrichment, aggregations, modeling, statistical summarization, validation and data quality testing</td>
        </tr>
        <tr>
          <td><b>Serving</b></td>
          <td>Transformed data must be delivered to stakeholders or applications for actual use. This can involve feeding BI dashboards, analytics platforms, ML models, or external systems via APIs or reverse ETL for operational analytics</td>
          <td>Providing data to BI dashboards, analytics platforms, or reporting tools; feeding machine learning models; supplying external systems via APIs or reverse ETL; ensuring reliability, freshness, and security for all consumers</td>
        </tr>
        <tr>
          <td><b>Undercurrents</b></td>
          <td>Several critical themes run through all stages of the data engineering lifecycle, including security, data management, DataOps, data architecture, orchestration, and software engineering best practices</td>
          <td>
            <ul>
              <li><b>Security</b>: Implementing robust access controls, encryption, and compliance measures</li>
              <li><b>Data Management</b>:Establishing governance frameworks, metadata management, and data cataloging</li>
              <li><b>DataOps</b>:Adopting DevOps principles for data workflows</li>
              <li><b>Data Architecture</b>: Designing scalable architectures</li>
              <li><b>Orchestration</b>: Coordinating complex workflows</li>
              <li><b>Software Engineering</b>: Applying best practices in coding, version control, and documentation</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>

  </TabItem>
  <TabItem value="data-access" label="Data Access">
    <Tabs queryString="secondary">
      <TabItem value="data-access-frequency" label="Access Frequency" attributes={{ className: "tabs__vertical" }}>
        <table>
            <thead>
                <tr>
                    <th>Aspect</th>
                    <th>Hot Data</th>
                    <th>Lukewarm (Warm) Data</th>
                    <th>Cold Data</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Definition</td>
                    <td>Frequently accessed, high-value, real-time or near-real-time data</td>
                    <td>Moderately accessed, regularly needed but not instant</td>
                    <td>Infrequently accessed, usually retained for archival purposes</td>
                </tr>
                <tr>
                    <td>Access Frequency</td>
                    <td>Constant, immediate, sub-second or millisecond response</td>
                    <td>Scheduled, hours to days, moderate latency</td>
                    <td>Rare, weeks to years, high latency</td>
                </tr>
                <tr>
                    <td>Access Latency</td>
                    <td>Sub-second or millisecond</td>
                    <td>Seconds to minutes</td>
                    <td>Minutes to hours</td>
                </tr>
                <tr>
                    <td>Storage Media</td>
                    <td>RAM, in-memory database, SSDs, high-performance NAS</td>
                    <td>Mid-tier SSDs, high-speed HDDs, cloud object storage</td>
                    <td>Low-cost HDDs, archival cloud storage</td>
                </tr>
                <tr>
                    <td>Retention Policy</td>
                    <td>Short-term, transactional</td>
                    <td>Weeks to months, operational</td>
                    <td>Long-term, years (or indefinitely for compliance)</td>
                </tr>
                <tr>
                    <td>Data Volume</td>
                    <td>Typically smaller, volume managed for speed</td>
                    <td>Medium</td>
                    <td>Very large, bulk data</td>
                </tr>
                <tr>
                    <td>Data Value</td>
                    <td>Immediate, high business impact</td>
                    <td>Useful, moderate business impact</td>
                    <td>Historical, regulatory, analytical</td>
                </tr>
                <tr>
                    <td>Security Requirements</td>
                    <td>Highest, critical for business operations</td>
                    <td>Moderate, standard access protection</td>
                    <td>Encryption, integrity, regulatory compliance</td>
                </tr>
                <tr>
                    <td>Scalability</td>
                    <td>Vertical scaling for speed</td>
                    <td>Horizontal scaling, cost-performance balance</td>
                    <td>Massive horizontal scaling, low access needs</td>
                </tr>
                <tr>
                    <td>Challenges</td>
                    <td>High cost, data lifecycle, scalability</td>
                    <td>Balancing cost and access</td>
                    <td>Retrieval speed, data integrity, long-term maintenance</td>
                </tr>
                <tr>
                    <td>Use Cases</td>
                    <td>Fraud detection, real-time stock trading, network monitoring</td>
                    <td>Monthly business reporting, operational data</td>
                    <td>Legal audits, disaster recovery, regulatory reporting</td>
                </tr>
            </tbody>
        </table>
      </TabItem>
      <TabItem value="data-structured-vs-unstructured" label="Structured vs. Unstructured">
        <table>
            <thead>
                <tr>
                    <th>Aspect</th>
                    <th>Structured Data</th>
                    <th>Unstructured Data</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Definition</td>
                    <td>Data that adheres to a predefined schema or model, organized in rows and columns</td>
                    <td>Data that does not have a predefined structure or schema, often text-heavy or multimedia</td>
                </tr>
                <tr>
                    <td>Storage</td>
                    <td>Stored in relational databases (SQL)</td>
                    <td>Stored in NoSQL databases, data lakes, file systems</td>
                </tr>
                <tr>
                    <td>Data Model</td>
                    <td>Predefined schema with fixed fields and data types</td>
                    <td>No fixed schema; flexible and dynamic formats</td>
                </tr>
                <tr>
                    <td>Querying</td>
                    <td>Easily queried using SQL with structured queries</td>
                    <td>Requires specialized tools (e.g., text mining, image recognition) for analysis</td>
                </tr>
                <tr>
                    <td>Analysis Complexity</td>
                    <td>Straightforward analysis using traditional BI tools</td>
                    <td>Complex analysis often requiring machine learning or advanced analytics techniques</td>
                </tr>
                <tr>
                    <td>Volume</td>
                    <td>Typically smaller in volume due to structured nature</td>
                    <td>Larger in volume due to diverse formats and types</td>
                </tr>
                <tr>
                    <td>Processing Speed</td>
                    <td>Faster processing due to defined structure and indexing</td>
                    <td>Slower processing due to need for parsing and interpretation</td>
                </tr>
                <tr>
                    <td>Use Cases</td>
                    <td>Financial transactions, customer records, inventory management</td>
                    <td>Customer feedback analysis, media content management, sentiment analysis</td>
                </tr>
                <tr>
                    <td>Examples</td>
                    <td>Relational databases, spreadsheets, CSV files</td>
                    <td>Emails, social media posts, images, videos, audio files, documents (PDFs, Word)</td>
                </tr>
            </tbody>
        </table>
      </TabItem>
    </Tabs>
  </TabItem>
  <TabItem value="data-storage" label="Data Storage">
    <Tabs queryString="secondary">
      <TabItem value="data-collection" label="Data Collection" attributes={{className: 'tabs__vertical'}}>
        ## Bounded vs. Unbounded Data

        <table>
          <thead>
            <tr>
              <th>Aspect</th>
              <th>Bounded Data</th>
              <th>Unbounded Data</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Definition</td>
              <td>Finite data set with a known start and end point</td>
              <td>Infinite or continuously growing data with no predefined end</td>
            </tr>
            <tr>
              <td>Data Characteristics</td>
              <td>Fixed size, complete, and unchanging once fully collected</td>
              <td>Potentially infinite, dynamic, and continuously generated</td>
            </tr>
            <tr>
              <td>Examples</td>
              <td>Historical sales data, completed dataset for a specific period (e.g., last quarter sales)</td>
              <td>Streaming logs, real-time sensor data, social media feeds</td>
            </tr>
            <tr>
              <td>Processing Model</td>
              <td>Batch processing  -  data processed as a whole after collection</td>
              <td>Stream processing  -  data processed incrementally as it arrives</td>
            </tr>
            <tr>
              <td>Data Ordering</td>
              <td>Typically sequential and complete, allowing deterministic processing</td>
              <td>May be out-of-order, delayed, or non-sequential due to latency and distributed sources</td>
            </tr>
            <tr>
              <td>Timing</td>
              <td>Processed after data collection, often with latency (days, hours)</td>
              <td>Processed in real-time or near real-time with minimal delay</td>
            </tr>
            <tr>
              <td>System Architectures</td>
              <td>Traditional Data Warehouses, ETL pipelines, batch-oriented systems</td>
              <td>Streaming platforms like Apache Kafka, Apache Flink, Apache Beam, Spark Streaming</td>
            </tr>
            <tr>
              <td>Storage Requirements</td>
              <td>Larger storage upfront for entire dataset storage</td>
              <td>Continuous storage needs with potential for state management or windowing to handle data volume</td>
            </tr>
            <tr>
              <td>Computation Model</td>
              <td>Deterministic and re-runnable computations on fixed data sets</td>
              <td>Incremental, stateful computations with approximate processing or windowing to manage infinite data</td>
            </tr>
            <tr>
              <td>System Complexity</td>
              <td>Lower complexity in handling data consistency and completeness</td>
              <td>Higher complexity to handle out-of-order events, late data, and exactly-once processing guarantees</td>
            </tr>
            <tr>
              <td>Error Handling</td>
              <td>Errors can be corrected in batch runs before analysis</td>
              <td>Needs continuous monitoring and corrective mechanisms to handle anomalies in a live stream</td>
            </tr>
            <tr>
              <td>Scalability Challenges</td>
              <td>Scalability mainly in storage and batch job execution</td>
              <td>Requires scalable infrastructure to handle continuous high-throughput data ingestion and processing</td>
            </tr>
            <tr>
              <td>Latency</td>
              <td>Higher latency acceptable due to batch processing nature</td>
              <td>Low latency required to provide timely insights or actions</td>
            </tr>
            <tr>
              <td>Architectural Patterns</td>
              <td>ETL, Lambda Architecture (batch layer dominant)</td>
              <td>Kappa Architecture, unified stream processing approach combining batch and stream</td>
            </tr>
            <tr>
              <td>Data Completeness</td>
              <td>Complete view of the dataset after processing</td>
              <td>Incomplete snapshots at any point, with evolving data as stream progresses</td>
            </tr>
            <tr>
              <td>Examples from Real World</td>
              <td>Financial reports for closed fiscal year; archived web logs</td>
              <td>Network packet captures; social media mentions; real-time transaction feeds</td>
            </tr>
            <tr>
              <td>Use Cases</td>
              <td>Historical analytics, reporting, compliance auditing</td>
              <td>Real-time analytics, alerting, fraud detection, IoT monitoring</td>
            </tr>
          </tbody>
        </table>

        ## Batch vs. Micro-Batch vs. Real-Time Processing

        <table>
          <thead>
            <tr>
              <th>Aspect</th>
              <th>Batch Processing</th>
              <th>Micro-Batch Processing</th>
              <th>Real-Time Processing</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><b>Definition</b></td>
              <td>Collects and stores data over a period, processing all at once</td>
              <td>Processes data in small batches at short, regular intervals</td>
              <td>Processes data immediately as it arrives, near-instantaneous</td>
            </tr>
            <tr>
              <td><b>Frequency</b></td>
              <td>Low frequency (e.g., hourly, daily, monthly)</td>
              <td>Medium frequency (seconds to minutes intervals)</td>
              <td>High frequency (sub-second to real-time continuous)</td>
            </tr>
            <tr>
              <td><b>Latency</b></td>
              <td>High latency due to waiting for batch completion</td>
              <td>Moderate latency, quicker than batch but not instantaneous</td>
              <td>Very low latency, near-instant results</td>
            </tr>
            <tr>
              <td><b>Data Volume</b></td>
              <td>Large volumes of accumulated data</td>
              <td>Smaller chunks of data per batch</td>
              <td>Continuous streams of individual events</td>
            </tr>
            <tr>
              <td><b>Complexity</b></td>
              <td>Simple to implement and manage</td>
              <td>Moderate complexity, combines batch and streaming elements</td>
              <td>High complexity requiring advanced architecture and tooling</td>
            </tr>
            <tr>
              <td><b>Resource Utilization</b></td>
              <td>Efficient resource use, runs during off-peak times</td>
              <td>More frequent resource use than batch, less than streaming</td>
              <td>Resource-intensive, requires horizontal scaling</td>
            </tr>
            <tr>
              <td><b>Processing Model</b></td>
              <td>Triggered by schedule (time or data volume)</td>
              <td>Triggered by time interval or data size threshold</td>
              <td>Constant event-driven processing</td>
            </tr>
            <tr>
              <td><b>Stateful Processing Support</b></td>
              <td>Yes, often requires stateful operations</td>
              <td>Supports small state, similar to batch</td>
              <td>Usually stateless or manages small state due to speed demand</td>
            </tr>
            <tr>
              <td><b>Data Freshness</b></td>
              <td>Lower data freshness, data available after processing batch</td>
              <td>Near real-time freshness, data is updated every few minutes</td>
              <td>Highest data freshness, updates data as it arrives</td>
            </tr>
            <tr>
              <td><b>Fault Tolerance</b></td>
              <td>Easier to handle failures with retries during next batch</td>
              <td>Moderate fault tolerance</td>
              <td>Requires robust fault tolerance mechanisms, checkpointing</td>
            </tr>
            <tr>
              <td><b>Typical Technologies</b></td>
              <td>Apache Spark batch jobs, Hadoop MapReduce</td>
              <td>Apache Spark Streaming, Fluentd, Logstash</td>
              <td>Apache Kafka, Apache Flink, Apache Pulsar</td>
            </tr>
            <tr>
              <td><b>Cost</b></td>
              <td>Lower operational cost due to infrequency</td>
              <td>Moderate operational costs</td>
              <td>Higher cost due to continuous processing and infrastructure</td>
            </tr>
            <tr>
              <td><b>Use Cases</b></td>
              <td>End-of-day reports, billing, historical analytics</td>
              <td>Incremental dashboard updates, near real-time user behavior</td>
              <td>Fraud detection, monitoring, live analytics</td>
            </tr>
          </tbody>
        </table>

        ## Pull vs. Push

        <table>
            <thead>
                <tr>
                    <th>Aspect</th>
                    <th>Pull</th>
                    <th>Push</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Initiation</td>
                    <td>Data target pulls data from the source by requesting it explicitly, as needed or at scheduled intervals</td>
                    <td>Data source initiates and sends data to the target automatically when data is available</td>
                </tr>
                <tr>
                    <td>Control of Flow</td>
                    <td>Target controls when and how much data to ingest (e.g., batch size, frequency)</td>
                    <td>Source controls the data flow; target has little control over rate or timing</td>
                </tr>
                <tr>
                    <td>Real-time Capability</td>
                    <td>Can be near-real-time but often involves periodic polling. Typically higher latency than push</td>
                    <td>Immediate; real-time delivery as soon as new data is generated</td>
                </tr>
                <tr>
                    <td>Scalability</td>
                    <td>Highly scalable; multiple consumers can independently fetch data at their own pace, easier replication, supports distributed scaling</td>
                    <td>May overwhelm consumers if the source produces more data than the targets can handle; hard to optimize for multiple consumers</td>
                </tr>
                <tr>
                    <td>Replayability/Recovery</td>
                    <td>Easier to recover or reprocess missed data, as the consumer can retry requests or fetch from specific offsets</td>
                    <td>Replay is challenging - if a consumer misses data, it's hard to get the missing pieces back unless a buffer or queue is used</td>
                </tr>
                <tr>
                    <td>Latency</td>
                    <td>May introduce latency depending on polling frequency and network delays</td>
                    <td>Low latency; pushes changes to consumers as soon as available</td>
                </tr>
                <tr>
                    <td>Efficiency</td>
                    <td>May require more bandwidth for frequent polling; less efficient for frequent changes unless optimized</td>
                    <td>Efficient for sources with frequent changes or high update rates - useful for event-driven architectures</td>
                </tr>
                <tr>
                    <td>Security</td>
                    <td>Target must connect to source, requiring bidirectional communication and b security layers on the source</td>
                    <td>More secure for sources; the source doesn't listen for network connections, reducing attack surfaces</td>
                </tr>
                <tr>
                    <td>Operational Complexity</td>
                    <td>Source must allow for external requests, potentially more firewall and authentication setup; simpler consumer-side error handling and scaling</td>
                    <td>Potentially less operational overhead if rate limiting and buffering are handled; but flow control and backpressure management are harder</td>
                </tr>
                <tr>
                    <td>Data Ownership</td>
                    <td>Consumer chooses what, when, and how much to ingest, offering flexibility for diverse requirements</td>
                    <td>Source knows and manages its own data, ensuring accurate and robust delivery</td>
                </tr>
                <tr>
                    <td>Implementation Details</td>
                    <td>Requires periodic scheduler or polling mechanism; easier integration with existing APIs and systems</td>
                    <td>Requires consumers to implement logic for handling unsolicited data, queuing, or buffering; flow and rate limiting complex</td>
                </tr>
                <tr>
                    <td>Hybrid Patterns</td>
                    <td>Hybrid approaches leverage strengths of both, such as push for immediate updates and pull for detailed/batch data</td>
                    <td>Often combined - e.g., system pushes notifications but clients pull detailed data as needed</td>
                </tr>
                <tr>
                    <td>Consistency Guarantees</td>
                    <td>Easier to achieve exactly-once or at-least-once semantics with systems like Kafka</td>
                    <td>Needs careful orchestration for b consistency, especially in distributed setups</td>
                </tr>
                <tr>
                    <td>Common Technologies</td>
                    <td>REST APIs, scheduled ETL, database dumps, Kafka Connect, batch queries, polling</td>
                    <td>Webhooks, real-time streaming (e.g., MQTT), proprietary push APIs, some ETL tools</td>
                </tr>
                <tr>
                    <td>Use Cases</td>
                    <td>Reporting, batch processing, periodic data sync, data lakes, less time-sensitive operations</td>
                    <td>Time-sensitive, high-frequency events, IoT devices, notification systems, or real-time analytics</td>
                </tr>
            </tbody>
        </table>
      </TabItem>
      <TabItem value="data-modeling" label="Data Modeling">
        <table className="text_vertical">
          <thead>
            <tr>
              <th>Data Modeling Technique</th>
              <th>Definition</th>
              <th>Characteristics</th>
              <th>Advantages</th>
              <th>Disadvantages</th>
              <th>Use Cases</th>
              <th>Examples</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Conceptual Data Modeling</td>
              <td>High-level, abstract model focusing on business entities and relationships without technical detail</td>
              <td>Entities and relationships shown typically via ER diagrams or UML class diagrams</td>
              <td>Platform-neutral, easy communication with business stakeholders</td>
              <td>Lacks technical detail for implementation</td>
              <td>Business planning, stakeholder alignment, early project stages</td>
              <td>ER Diagrams, UML</td>
            </tr>
            <tr>
              <td>Logical Data Modeling</td>
              <td>Detailed model defining data elements, attributes, relationships, keys, and rules without platform dependency</td>
              <td>Normalized tables, keys, constraints, and relationships; focus on data integrity and structure</td>
              <td>Provides clean, normalized design; aids data quality and governance</td>
              <td>Does not address physical storage, indexing, or performance optimization</td>
              <td>Schema design, data governance, preparing for physical modeling</td>
              <td>3NF, Data Vault modeling (hubs-links-satellites)</td>
            </tr>
            <tr>
              <td>Physical Data Modeling</td>
              <td>Model optimized for database implementation specifying tables, columns, indexes, partitions, etc</td>
              <td>Denormalized/normalized tables, platform-specific constructs like indexes, partitions, cluster keys</td>
              <td>Optimizes storage, retrieval, and query performance</td>
              <td>Tightly coupled to specific technologies, less flexible</td>
              <td>Performance tuning and optimization for specific DB engines</td>
              <td>Star schema, Snowflake schema, Anchor Modeling</td>
            </tr>
            <tr>
              <td>Dimensional Modeling</td>
              <td>Simplifies data structures for analytical queries grouping data into facts and dimensions</td>
              <td>Fact tables (numeric measures) linked to dimension tables (contextual descriptors) like star or snowflake schema</td>
              <td>Intuitive for analysts, improves query speed for OLAP workloads</td>
              <td>Less suitable for transactional systems, more redundancy</td>
              <td>BI, data warehouses, dashboards, self-service analytics</td>
              <td>Star Schema, Snowflake Schema, Slowly Changing Dimensions (SCDs)</td>
            </tr>
            <tr>
              <td>Relational Data Modeling</td>
              <td>Organizes data in normalized tables ensuring minimal redundancy and data consistency</td>
              <td>Tables with rows and columns, defined primary and foreign keys, normalized forms</td>
              <td>Strong data integrity, widely supported, good for complex relationships</td>
              <td>Can be complex to query for analytical workloads, performance overhead for joins</td>
              <td>OLTP systems, master data management, transactional apps</td>
              <td>2NF, 3NF, Boyce-Codd Normal Form (BCNF), ER diagrams</td>
            </tr>
            <tr>
              <td>Entity-Relationship (ER) Model</td>
              <td>Represents entities, attributes, and their relationships for database design</td>
              <td>Entities represented as objects/tables; attributes as columns; relationships with cardinality and optionality</td>
              <td>Clear visualization of data relationships, promotes normalization</td>
              <td>May become complex for large systems, design only</td>
              <td>Database design, relational databases</td>
              <td>Chen ER Model, Crow's Foot notation</td>
            </tr>
            <tr>
              <td>Object-Oriented Data Modeling</td>
              <td>Combines data with behavior, encapsulates data and operations together representing objects</td>
              <td>Objects with attributes and methods; supports inheritance, classes, and polymorphism</td>
              <td>Closer to real-world modeling, reusable components</td>
              <td>Complexity, less common in traditional DBs</td>
              <td>Object databases, applications using OOP principles</td>
              <td>Classes, inheritance hierarchies</td>
            </tr>
            <tr>
              <td>Hierarchical Data Modeling</td>
              <td>Organizes data in tree-like parent-child relationships</td>
              <td>Strict one-to-many parent-child relationships; records organized in a hierarchy</td>
              <td>Simple and fast navigation in one-to-many data</td>
              <td>Inflexible with many-to-many or complex relationships</td>
              <td>Legacy systems, XML/JSON document stores, file systems</td>
              <td>IMS database, XML schemas</td>
            </tr>
            <tr>
              <td>Network Data Modeling</td>
              <td>Extends hierarchical to allow many-to-many relationships</td>
              <td>Graph-like structures, records with multiple owners or parents</td>
              <td>More flexible than hierarchical, models complex relationships</td>
              <td>More complex design and management than relational</td>
              <td>Complex interconnected data like telecommunications, logistics</td>
              <td>CODASYL, Graph databases (Neo4j, Amazon Neptune)</td>
            </tr>
            <tr>
              <td>Temporal/Historical Modeling</td>
              <td>Tracks data changes over time for auditing, historical analysis</td>
              <td>Stores multiple data versions with timestamps for valid and transaction time</td>
              <td>Supports full data history and versioning, improves auditability</td>
              <td>Increases data storage and complexity</td>
              <td>Compliance, time-series, audit trails, customer lifecycle</td>
              <td>Bitemporal modeling, Slowly Changing Dimensions (SCDs), Anchor modeling</td>
            </tr>
            <tr>
              <td>Agile Data Modeling</td>
              <td>Enables iterative and flexible modeling adapting to evolving business needs</td>
              <td>Combines techniques, emphasizes collaboration and incremental updates</td>
              <td>Highly adaptable, incorporates feedback quickly</td>
              <td>Can lack initial rigor, may lead to inconsistent models</td>
              <td>Rapid development environments, evolving business domains</td>
              <td>Often combined with other models in Agile projects</td>
            </tr>
            <tr>
              <td>Big Data Modeling</td>
              <td>Tailored to handle volume, velocity, and variety of big data</td>
              <td>May use NoSQL schema-on-read, data lakes, schemas for semi-structured data</td>
              <td>Scales for huge data volumes, flexible schema</td>
              <td>Less mature standards, potential for data inconsistency</td>
              <td>Big data platforms, streaming analytics</td>
              <td>Schema-on-read, Hadoop, NoSQL, Data lakehouse</td>
            </tr>
            <tr>
              <td>Inmon</td>
              <td>Corporate Information Factory (CIF). Enterprise-wide data architecture integrating various data sources into a centralized warehouse. Flow: Sources → Staging (ETL) → Enterprise Data Warehouse (Data stored in 3NF) → Data Marts → Consumption</td>
              <td>Top-down approach, normalized data warehouse, data marts for specific domains</td>
              <td>Comprehensive, consistent enterprise view</td>
              <td>Complex, time-consuming implementation</td>
              <td>Large enterprises needing integrated data</td>
              <td>Normalized data warehouse, data marts</td>
            </tr>
            <tr>
              <td>Kimball</td>
              <td>Bus Architecture. Dimensional modeling approach focusing on ease of use and performance. Flow: Sources → Staging (ETL) → Enterprise Data Warehouse (STAR Shema) → Data Marts → Consumption</td>
              <td>Bottom-up approach, data marts for specific business areas</td>
              <td>Fast query performance, user-friendly data structures</td>
              <td>Can lead to data silos, less comprehensive view</td>
              <td>Mid-sized to large enterprises with specific reporting needs</td>
              <td>Star schema, snowflake schema, data marts</td>
            </tr>
            <tr>
              <td>Data Vault (Linstedt)</td>
              <td>Hybrid approach combining elements of 3NF and dimensional modeling. Flow: Sources → Staging (ETL) → Raw Data Vault → Business Data Vault → Data Marts → Consumption</td>
              <td>Focuses on agility and scalability, accommodating changes easily</td>
              <td>Supports historical tracking and auditability</td>
              <td>Can be complex to implement and manage</td>
              <td>Organizations needing flexibility and rapid change adaptation</td>
              <td>Data vault model, hubs, links, satellites</td>
            </tr>
          </tbody>
        </table>
      </TabItem>
      <TabItem value="slowly-changing-dimension" label="Slowly Changing Dimension">
        Slowly Changing Dimensions (SCDs) are dimension tables in data warehouses where attribute values change slowly over time. Unlike frequently changing fact data, dimension data (e.g., customer details, product attributes) requires historical tracking for accurate reporting. SCDs manage these changes in various ways to meet different business and analysis needs.

        <table className="text_vertical">
          <thead>
              <tr>
                  <th>Aspect</th>
                  <th>Type 0: Retain Original</th>
                  <th>Type 1: Overwrite</th>
                  <th>Type 2: Add New Row</th>
                  <th>Type 3: Add New Attribute</th>
                  <th>Type 4: Add History Table</th>
                  <th>Type 5: Add Mini-Dimension</th>
                  <th>Type 6: Combined Approach</th>
                  <th>Type 7: Hybrid Approach</th>
              </tr>
          </thead>
          <tbody>
              <tr>
                  <td>Description</td>
                  <td>Attribute never changes; always original value</td>
                  <td>Overwrite old data; no history kept</td>
                  <td>Insert new row for each change; full history tracked</td>
                  <td>Add new column(s) to track limited previous value</td>
                  <td>Store historical data in separate history table</td>
                  <td>Create a mini-dimension for frequently changing attributes</td>
                  <td>Combines Types 1, 2, 3 in one model (overwrite, add row, add attribute)</td>
                  <td>Combines various SCD techniques beyond Type 6 for adaptive needs</td>
              </tr>
              <tr>
                  <td>Visualization</td>
                  <td>
                    The attribute never changes, so the entity design simply holds the original column with no alteration

                    ```mermaid
                    erDiagram
                      CUSTOMER {
                          int id
                          string name
                          date date_of_birth  "Type 0: Never changes"
                      }
                    ```
                  </td>
                  <td>
                    Changes overwrite old values, and there is no history kept

                    ```mermaid
                    erDiagram
                        CUSTOMER {
                            int id
                            string name
                            string email "Type 1: Always latest value"
                        }
                    ```
                  </td>
                  <td>
                    New row is added for each change, keeping full history. Start and end dates track consistency

                    ```mermaid
                    erDiagram
                        CUSTOMER {
                            int surrogate_key
                            int customer_id
                            string name
                            string address
                            date valid_from
                            date valid_to
                            bool is_current
                        }
                    ```
                  </td>
                  <td>
                    One or more additional columns retain limited history (e.g., previous value)

                    ```mermaid
                    erDiagram
                        CUSTOMER {
                            int id
                            string name
                            string current_address "Type 3: current"
                            string previous_address "Type 3: previous"
                        }
                    ```
                  </td>
                  <td>
                    Separate history table is created to maintain full change history, keeping the current state in the main dimension

                    ```mermaid
                    erDiagram
                        CUSTOMER {
                            int id
                            string name
                            string current_address
                        }
                        CUSTOMER_HISTORY {
                            int history_id
                            int customer_id
                            string name
                            string address
                            date changed_on
                        }
                        CUSTOMER ||--o{ CUSTOMER_HISTORY : tracks
                    ```
                  </td>
                  <td>
                    Mini-dimension stores rapidly changing attributes separately, referenced by the main dimension

                    ```mermaid
                    erDiagram
                        CUSTOMER {
                            int id
                            string name
                            int mini_dim_id
                        }
                        MINI_DIM {
                            int mini_dim_id
                            string preference
                            string status
                        }
                        CUSTOMER }o--|| MINI_DIM : references
                    ```
                  </td>
                  <td>
                    Combines Types 1, 2, and 3. Maintains both current values (Type 1) and full history (Type 2) and a previous attribute (Type 3)

                    ```mermaid
                    erDiagram
                        CUSTOMER {
                            int surrogate_key
                            int customer_id
                            string name
                            string address_current "Type 1 (latest)"
                            string address_previous "Type 3 (previous)"
                            date valid_from
                            date valid_to
                            bool is_current
                        }
                    ```
                  </td>
                  <td>
                    Flexible hybrid approach, often combining multiple SCD strategies for different columns depending on business requirements

                    ```mermaid
                    erDiagram
                        CUSTOMER {
                            int surrogate_key "For Type 2 tracking"
                            int customer_id
                            string name
                            string address "Type 1 or Type 2 as needed"
                            string status_code "Could be Type 1 or 3"
                            int mini_dim_id "For Type 5 links"
                            date valid_from
                            date valid_to
                            bool is_current
                        }
                        MINI_DIM {
                            int mini_dim_id
                            string attr1
                            string attr2
                        }
                        CUSTOMER }o--|| MINI_DIM : mini_dimension
                    ```
                  </td>
              </tr>
              <tr>
                  <td>Change Handling Method</td>
                  <td>No update</td>
                  <td>Overwrite existing values</td>
                  <td>Add new record per change</td>
                  <td>Add new column to track previous value</td>
                  <td>Use separate history table for old data</td>
                  <td>Extract frequently changing attributes into separate mini-dim table</td>
                  <td>Current and historical columns plus version column</td>
                  <td>Flexible, combines multiple change management techniques</td>
              </tr>
              <tr>
                  <td>Historical Data Tracking</td>
                  <td>No</td>
                  <td>No</td>
                  <td>Yes</td>
                  <td>Limited (only one previous value)</td>
                  <td>Yes</td>
                  <td>Partial history through mini-dims</td>
                  <td>Yes</td>
                  <td>Yes</td>
              </tr>
              <tr>
                  <td>Storage Impact</td>
                  <td>Minimal</td>
                  <td>Minimal</td>
                  <td>High (multiple rows per entity)</td>
                  <td>Moderate (additional columns)</td>
                  <td>Moderate to High (two tables)</td>
                  <td>Moderate (extra mini-dim tables)</td>
                  <td>High (due to multiple approaches combined)</td>
                  <td>Variable, depends on component types used</td>
              </tr>
              <tr>
                  <td>Query Complexity</td>
                  <td>Very simple</td>
                  <td>Simple</td>
                  <td>More complex due to multiple rows</td>
                  <td>Simple for limited history</td>
                  <td>Moderate due to joins with history table</td>
                  <td>Moderate (joins with mini-dim)</td>
                  <td>Moderate to complex</td>
                  <td>Complex, depending on combination used</td>
              </tr>
              <tr>
                  <td>Pros</td>
                  <td>Simple; fast queries</td>
                  <td>Easy implementation, fast update</td>
                  <td>Full historical data tracking</td>
                  <td>Easy access to current and prior value</td>
                  <td>Clear separation of current and historical data</td>
                  <td>Improves query performance for frequent small changes</td>
                  <td>Flexible; combines best of types 1, 2, 3</td>
                  <td>Highly adaptable to complex scenarios</td>
              </tr>
              <tr>
                  <td>Cons</td>
                  <td>No history, no ability to analyze change</td>
                  <td>History lost</td>
                  <td>Adds storage; may impact performance</td>
                  <td>Only tracks limited history, not scalable</td>
                  <td>Extra complexity with multiple tables</td>
                  <td>Additional ETL and dimensional complexity</td>
                  <td>Complexity; maintenance overhead</td>
                  <td>High complexity; requires sophisticated design</td>
              </tr>
              <tr>
                  <td>Implementation Complexity</td>
                  <td>Low</td>
                  <td>Low</td>
                  <td>Moderate to high</td>
                  <td>Low to moderate</td>
                  <td>Moderate to high</td>
                  <td>Moderate to high</td>
                  <td>High</td>
                  <td>Very high</td>
              </tr>
              <tr>
                  <td>Impact on Performance</td>
                  <td>Minimal</td>
                  <td>Minimal</td>
                  <td>Can degrade with large historical data</td>
                  <td>Moderate</td>
                  <td>Moderate</td>
                  <td>Moderate</td>
                  <td>Can be performance intensive</td>
                  <td>Depends on implemented hybrid techniques</td>
              </tr>
              <tr>
                  <td>Dimension Table Action</td>
                  <td>No change to attribute value</td>
                  <td>Overwrite attribute value</td>
                  <td>Add new dimension row for profile with new attribute value</td>
                  <td>Add new column to preserve attribute's current and prior values</td>
                  <td>Add mini-dimension table containing rapidly changing attributes</td>
                  <td>Add type 4 mini-dimension, along with overwritten type 1 mini-dimension key in base dimension</td>
                  <td>Add type 1 overwritten attributes to type 2 dimension row, and overwrite all prior dimension rows</td>
                  <td>Add type 2 dimension row with new attribute value, plus view limited to current rows and/or attribute values</td>
              </tr>
              <tr>
                  <td>Impact on Fact Analysis</td>
                  <td>Facts associated with attribute's original value</td>
                  <td>Facts associated with attribute's current value</td>
                  <td>Facts associated with attribute value in effect when fact occurred</td>
                  <td>Facts associated with both current and prior attribute alternative values</td>
                  <td>Facts associated with rapidly changing attributes in effect when fact occurred</td>
                  <td>Facts associated with rapidly changing attributes in effect when fact occurred, plus current rapidly changing attribute values</td>
                  <td>Facts associated with attribute value in effect when fact occurred, plus current values</td>
                  <td>Facts associated with attribute value in effect when fact occurred, plus current values</td>
              </tr>
              <tr>
                  <td>Use Cases</td>
                  <td>Static attributes like SSN, zip codes</td>
                  <td>Correcting typos, non-critical updates e.g. email, phone</td>
                  <td>Track full history of customer address, employee job changes</td>
                  <td>Track current and previous salary, status</td>
                  <td>Maintain full historical pricing, employment data</td>
                  <td>Track attributes like customer segmentation that change frequently</td>
                  <td>Employee role and department tracking with full change history</td>
                  <td>Complex enterprise needs, combining multiple SCD styles</td>
              </tr>
          </tbody>
        </table>
      </TabItem>
      <TabItem value="schemas" label="Schemas">

      ## Schema Types

      <table>
        <thead>
          <tr>
            <th>Aspect</th>
            <th>Physical Schema</th>
            <th>Logical Schema</th>
            <th>Evolving Schema</th>
            <th>Contractual Schema (API)</th>
            <th>Metadata Schema</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Definition</td>
            <td>Describes how data is physically stored and arranged (files, indices, partitions) on storage media or DBMS</td>
            <td>Defines the logical (human-readable) structure: tables, fields, relationships, constraints</td>
            <td>Captures the actual schema changes (add/remove fields) over time, typically in dynamic or pipeline-driven systems</td>
            <td>Schema defining fields and their validation between systems via an API contract (e.g., JSON, GraphQL)</td>
            <td>Schema that describes the data about data, such as lineage, column descriptions, and governance</td>
          </tr>
          <tr>
            <td>Level of Abstraction</td>
            <td>Lowest: hardware, file system, storage block level</td>
            <td>Higher: data model, independent of storage</td>
            <td>Variable: follows either physical or logical but adapts to change</td>
            <td>Variable: can be logical or physical depending on API implementation</td>
            <td>Varies: may refer to logical, physical, or conceptual layers</td>
          </tr>
          <tr>
            <td>Focus</td>
            <td>Performance, storage efficiency, physical locations</td>
            <td>Data organization, integrity, relationships, constraints</td>
            <td>Handling schema drift, flexibility for changes</td>
            <td>Interface definition, data validation, compatibility</td>
            <td>Data governance, lineage, quality, observability</td>
          </tr>
          <tr>
            <td>Typical Stakeholders</td>
            <td>DBAs, infrastructure engineers</td>
            <td>Data modelers, analysts, architects</td>
            <td>Data engineers, analytics teams</td>
            <td>Backend engineers, API consumers/producers</td>
            <td>Data governance, compliance, data stewards</td>
          </tr>
          <tr>
            <td>Benefits</td>
            <td>Maximizes storage & query performance; supports tuning, scaling</td>
            <td>Ensures consistency, maintainability, integrity of business logic</td>
            <td>Enables rapid evolution, tracks change, minimizes disruption</td>
            <td>Allows machine interoperability, enforces standards, prevents breakage</td>
            <td>Aids data discovery, quality, lineage, and regulatory compliance</td>
          </tr>
          <tr>
            <td>Limitations</td>
            <td>Complex to change, tightly-coupled to hardware/DBMS</td>
            <td>May hide physical inefficiencies, less relevant for storage choices</td>
            <td>Risk of data loss or incompatibility if not managed well</td>
            <td>Tight coupling can hinder API flexibility, requires documentation</td>
            <td>Can become outdated or incomplete without good processes</td>
          </tr>
          <tr>
            <td>Use Cases</td>
            <td>DBMS optimization, partitions, indexes, backup/recovery strategies</td>
            <td>ER diagrams, database normalization, data modeling</td>
            <td>ELT pipelines, analytics, SaaS product changes</td>
            <td>API design, system integration, microservices communication</td>
            <td>Data catalogs, pipeline documentation, lineage tracking</td>
          </tr>
          <tr>
            <td>Examples</td>
            <td>Parquet files with partitioning; index files for tables; disk layouts</td>
            <td>Star schema; ERD; relational database definitions</td>
            <td>Adding new analytics events; updating field names in ELT</td>
            <td>REST/GraphQL/OpenAPI schema definitions; JSON schema</td>
            <td>dbt sources.yml; OpenMetadata; catalog records; lineage graphs</td>
          </tr>
        </tbody>
      </table>

      ## Star vs. Snowflake vs. Galaxy Schema

      <table>
          <thead>
              <tr>
                  <th>Aspect</th>
                  <th>Star Schema</th>
                  <th>Snowflake Schema</th>
                  <th>Galaxy Schema</th>
              </tr>
          </thead>
          <tbody>
              <tr>
                  <td>Structure</td>
                  <td>Central fact table linked to denormalized dimension tables</td>
                  <td>Fact table linked to normalized dimension tables, split hierarchically</td>
                  <td>Multiple fact tables sharing dimension tables, can be a mix of star and snowflake</td>
              </tr>
              <tr>
                  <td>Visualization</td>
                  <td>
                    ```mermaid
                    erDiagram
                        FACT_SALES ||--o{ DIM_CUSTOMER : "belongs to"
                        FACT_SALES ||--o{ DIM_PRODUCT : "contains"
                        FACT_SALES ||--o{ DIM_DATE : "occurs on"
                        FACT_SALES ||--o{ DIM_STORE : "happens at"

                        FACT_SALES {
                            int sale_id
                            int customer_id
                            int product_id
                            int date_id
                            int store_id
                            float sale_amount
                            int quantity
                        }
                        DIM_CUSTOMER {
                            int customer_id
                            string customer_name
                            string gender
                            string address
                        }
                        DIM_PRODUCT {
                            int product_id
                            string product_name
                            string category
                            string brand
                        }
                        DIM_DATE {
                            int date_id
                            date full_date
                            string day_of_week
                            int month
                            int year
                        }
                        DIM_STORE {
                            int store_id
                            string store_name
                            string city
                            string state
                        }
                    ```
                  </td>
                  <td>
                    ```mermaid
                    erDiagram
                        FACT_SALES ||--o{ DIM_CUSTOMER : "belongs to"
                        FACT_SALES ||--o{ DIM_PRODUCT : "contains"
                        FACT_SALES ||--o{ DIM_DATE : "occurs on"
                        FACT_SALES ||--o{ DIM_STORE : "happens at"

                        DIM_CUSTOMER ||--o{ DIM_GEOGRAPHY : "lives in"
                        DIM_PRODUCT ||--o{ DIM_CATEGORY : "categorized as"
                        DIM_PRODUCT ||--o{ DIM_BRAND : "manufactured by"
                        DIM_STORE ||--o{ DIM_CITY : "located in"
                        DIM_CITY ||--o{ DIM_STATE : "part of"

                        FACT_SALES {
                            int sale_id
                            int customer_id
                            int product_id
                            int date_id
                            int store_id
                            float sale_amount
                            int quantity
                        }
                        DIM_CUSTOMER {
                            int customer_id
                            string customer_name
                            string gender
                            int geography_id
                        }
                        DIM_GEOGRAPHY {
                            int geography_id
                            string country
                            string region
                        }
                        DIM_PRODUCT {
                            int product_id
                            string product_name
                            int category_id
                            int brand_id
                        }
                        DIM_CATEGORY {
                            int category_id
                            string category_name
                        }
                        DIM_BRAND {
                            int brand_id
                            string brand_name
                        }
                        DIM_DATE {
                            int date_id
                            date full_date
                            string day_of_week
                            int month
                            int year
                        }
                        DIM_STORE {
                            int store_id
                            string store_name
                            int city_id
                        }
                        DIM_CITY {
                            int city_id
                            string city_name
                            int state_id
                        }
                        DIM_STATE {
                            int state_id
                            string state_name
                        }
                    ```
                  </td>
                  <td>
                    ```mermaid
                    erDiagram
                    DIM_DATE {
                      string DateKey PK "Primary Key"
                      date FullDate
                      string DayOfWeek
                      string Month
                      string Quarter
                      string Year
                    }
                    DIM_PRODUCT {
                      string ProductKey PK "Primary Key"
                      string ProductName
                      string Category
                      string Brand
                    }
                    DIM_STORE {
                      string StoreKey PK "Primary Key"
                      string StoreName
                      string StoreLocation
                      string StoreManager
                    }
                    DIM_CUSTOMER {
                      string CustomerKey PK "Primary Key"
                      string FirstName
                      string LastName
                      string Gender
                      string Email
                    }

                    FACT_SALES {
                        string SalesID PK "Primary Key"
                        string DateKey FK
                        string ProductKey FK
                        string StoreKey FK
                        string CustomerKey FK
                        int QuantitySold
                        decimal SalesAmount
                    }
                    FACT_INVENTORY {
                        string InventoryID PK "Primary Key"
                        string DateKey FK
                        string ProductKey FK
                        string StoreKey FK
                        int QuantityOnHand
                        decimal InventoryValue
                    }
                    FACT_SHIPPING {
                        string ShippingID PK "Primary Key"
                        string DateKey FK
                        string ProductKey FK
                        string StoreKey FK
                        string ShippingCarrier
                        int QuantityShipped
                        decimal ShippingCost
                    }

                    DIM_DATE ||--o{ FACT_SALES : "date of"
                    DIM_PRODUCT ||--o{ FACT_SALES : "product sold in"
                    DIM_STORE ||--o{ FACT_SALES : "store where sold"
                    DIM_CUSTOMER ||--o{ FACT_SALES : "customer buying"

                    DIM_DATE ||--o{ FACT_INVENTORY : "date recorded"
                    DIM_PRODUCT ||--o{ FACT_INVENTORY : "product in inventory"
                    DIM_STORE ||--o{ FACT_INVENTORY : "store inventory"

                    DIM_DATE ||--o{ FACT_SHIPPING : "shipping date"
                    DIM_PRODUCT ||--o{ FACT_SHIPPING : "product shipped"
                    DIM_STORE ||--o{ FACT_SHIPPING : "dispatch store"
                    ```
                  </td>
              </tr>
              <tr>
                  <td>Data Normalization</td>
                  <td>Dimension tables are denormalized (flat structure, redundancy present)</td>
                  <td>Dimension tables are normalized (data split into sub-tables, minimal redundancy)</td>
                  <td>Typically involves normalized or partially normalized dimension tables to reduce data redundancy. Dimensions are often conformed (shared across fact tables). Normalization level can vary depending on design goals</td>
              </tr>
              <tr>
                  <td>Query Performance</td>
                  <td>Faster query execution due to fewer joins</td>
                  <td>Slower query execution due to multiple joins required</td>
                  <td>Performance can vary; may benefit from fewer joins but could be impacted by complexity</td>
              </tr>
              <tr>
                  <td>Query Complexity</td>
                  <td>Simpler queries, fewer joins, easy to write and understand</td>
                  <td>More complex queries, requires deeper understanding and multiple joins</td>
                  <td>Queries can be complex due to multiple fact tables and shared dimensions; requires good understanding of schema</td>
              </tr>
              <tr>
                  <td>Storage Requirements</td>
                  <td>Higher storage use due to redundant and denormalized data</td>
                  <td>More storage efficient; reduced duplication through normalization</td>
                  <td>Storage efficiency varies; can be optimized through shared dimensions but may still have redundancy depending on design</td>
              </tr>
              <tr>
                  <td>Data Redundancy</td>
                  <td>Higher - dimensions repeat attribute values in multiple rows</td>
                  <td>Lower - most redundant data is eliminated</td>
                  <td>Varies - some redundancy may remain depending on design</td>
              </tr>
              <tr>
                  <td>Space Usage</td>
                  <td>More storage space required for large datasets</td>
                  <td>Less storage space through normalization</td>
                  <td>Varies - can be optimized but may still require significant space depending on data volume and design</td>
              </tr>
              <tr>
                  <td>Foreign Keys</td>
                  <td>Fewer foreign keys (simple design)</td>
                  <td>More foreign keys due to multiple related tables</td>
                  <td>Multiple foreign keys due to shared dimensions; complexity depends on design</td>
              </tr>
              <tr>
                  <td>Data Integrity</td>
                  <td>Lower: Denormalization risks inconsistency due to data being updated in many places</td>
                  <td>Higher: Normalization enforces referential integrity and consistency</td>
                  <td>Varies - can be managed but may require more effort to maintain consistency</td>
              </tr>
              <tr>
                  <td>Updates and Modifications</td>
                  <td>Harder to update - redundant data increases risk of inconsistent modifications</td>
                  <td>Easier for updates - changes in an attribute only affect one table</td>
                  <td>Varies - updates may be easier due to shared dimensions but can be complex depending on relationships</td>
              </tr>
              <tr>
                  <td>Dimension Table Structure</td>
                  <td>Flat structure - each dimension is a single table, no sub-tables</td>
                  <td>Multi-layered - each dimension may be decomposed into sub-dimensions</td>
                  <td>Varies - dimensions can be flat or multi-layered depending on design</td>
              </tr>
              <tr>
                  <td>BI & Reporting Suitability</td>
                  <td>Best for BI tools, dashboards, and quick ad hoc queries</td>
                  <td>Better for complex analytical queries, detailed reporting, and multidimensional analysis</td>
                  <td>Suitable for complex reporting needs involving multiple fact tables; requires good understanding of schema</td>
              </tr>
              <tr>
                  <td>Maintainability</td>
                  <td>Easier to maintain, intuitive design</td>
                  <td>More difficult to maintain, complex design</td>
                  <td>Varies - can be complex to maintain due to multiple fact tables and shared dimensions</td>
              </tr>
              <tr>
                  <td>Design Complexity</td>
                  <td>Easier and faster to design and implement</td>
                  <td>Requires careful design due to hierarchical splitting</td>
                  <td>Varies - can be complex to design depending on relationships and shared dimensions</td>
              </tr>
              <tr>
                  <td>Scalability</td>
                  <td>Scalable for typical analytic workloads, though can suffer performance issues at extreme scale due to redundancy</td>
                  <td>Good scalability, especially for complex and large-scale data with multiple hierarchies</td>
                  <td>Scalability varies; can handle complex data but may require careful design to avoid performance bottlenecks</td>
              </tr>
              <tr>
                  <td>ETL/ELT Complexity</td>
                  <td>Simpler ETL/ELT pipelines - fewer tables to populate and maintain</td>
                  <td>More complex ETL/ELT - hierarchical normalization requires careful loading and management</td>
                  <td>ETL/ELT complexity varies; may require more sophisticated pipelines to manage multiple fact tables and shared dimensions</td>
              </tr>
              <tr>
                  <td>Drawbacks</td>
                  <td>Data redundancy, storage waste, potential for inconsistencies, not suited for high-cardinality or complex hierarchies</td>
                  <td>Query slowness for basic analytics, complexity in query construction and ETL, harder for non-technical users to understand and navigate</td>
                  <td>Complexity in design and maintenance, potential performance issues if not well-optimized</td>
              </tr>
              <tr>
                  <td>Use Cases</td>
                  <td>Retail sales analysis with simple product/geography/time/customer dimensions</td>
                  <td>Data warehouses with complex product/customer/location hierarchies, and systems requiring fine-grained data integrity</td>
                  <td>Enterprise data warehouses with multiple business processes, complex reporting needs, and shared dimensions across fact tables</td>
              </tr>
          </tbody>
      </table>
      </TabItem>
    </Tabs>

  </TabItem>
  <TabItem value="data-architecture-patterns" label="Architecture Patterns">
    ## Lambda vs. Kappa

    <table>
      <thead>
        <tr>
          <th>Aspect</th>
          <th>Lambda</th>
          <th>Kappa</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Processing Model</td>
          <td>Combines batch processing and real-time stream processing in separate layers</td>
          <td>Uses a single, unified stream processing pipeline for both real-time and reprocessing</td>
        </tr>
        <tr>
          <td>Visualization</td>
          <td>
            ```mermaid
            graph LR
                subgraph Ingestion [Ingestion Layer]
                  service1(Service) e1@--> kafkaIngestion@{ shape: das, label: "Message Queue/Log" }
                  service2(Service N) e2@--> kafkaIngestion
                  cdc1[(CDC)] e3@--> kafkaIngestion
                  cdc2[(CDC N)] e4@--> kafkaIngestion
                end

                subgraph Speed [Speed Layer]
                  kafkaSpeed@{ shape: das, label: "Stream Processing" }
                end

                subgraph Batch [Batch Layer]
                  bucket[("Bucket")] e5@--> coldStorage[("Cold Storage")]
                end

                subgraph Serving [Serving Layer]
                  realTimeViews(Real-time Views)
                  batchViews(Batch Views)
                end

                kafkaIngestion e6@--> kafkaSpeed
                kafkaIngestion e7@--> bucket
                kafkaSpeed e8@--> Serving
                coldStorage e9@--> Serving

                e1@{ animate: true }
                e2@{ animate: true }
                e3@{ animate: true }
                e4@{ animate: true }
                e5@{ animate: true }
                e6@{ animate: true }
                e7@{ animate: true }
                e8@{ animate: true }
                e9@{ animate: true }
            ```
          </td>
          <td>
            ```mermaid
            graph LR
                subgraph Ingestion [Ingestion Layer]
                  service1(Service) e1@--> kafkaIngestion@{ shape: das, label: "Message Queue/Log" }
                  service2(Service N) e2@--> kafkaIngestion
                  cdc1[(CDC)] e3@--> kafkaIngestion
                  cdc2[(CDC N)] e4@--> kafkaIngestion
                end

                subgraph Speed [Speed Layer]
                  kafkaSpeed@{ shape: das, label: "Stream Processing" }
                  kafkaSpeed e5@--> bucket[("Bucket")]
                end

                subgraph Serving [Serving Layer]
                  realTimeViews(Real-time Views)
                  batchViews(Batch Views)
                end

                kafkaIngestion e6@--> kafkaSpeed
                kafkaSpeed e7@---> Serving

                e1@{ animate: true }
                e2@{ animate: true }
                e3@{ animate: true }
                e4@{ animate: true }
                e5@{ animate: true }
                e6@{ animate: true }
                e7@{ animate: true }
            ```
          </td>
        </tr>
        <tr>
          <td>Processing Layers</td>
          <td>Three layers: Batch Layer (large-scale processing), Speed Layer (real-time), Serving Layer (query)</td>
          <td>Single pipeline for all data, eliminating the batch layer</td>
        </tr>
        <tr>
          <td>Complexity</td>
          <td>High complexity; requires maintaining and synchronizing two separate codebases and pipelines</td>
          <td>Simpler architecture; only one processing pipeline to maintain</td>
        </tr>
        <tr>
          <td>Latency</td>
          <td>Batch layer processing introduces higher latency; speed layer offers low latency for real-time data</td>
          <td>Low latency overall due to continuous stream processing</td>
        </tr>
        <tr>
          <td>Fault Tolerance</td>
          <td>Fault tolerant: batch layer can recompute results if speed layer fails or produces errors</td>
          <td>Fault tolerant depending on stream processing reliability; relies on log replay for reprocessing errors</td>
        </tr>
        <tr>
          <td>Data Reprocessing Capability</td>
          <td>Batch layer enables accurate reprocessing of historical data to fix errors or recompute results</td>
          <td>Reprocessing done via replaying events from the log through the stream processor</td>
        </tr>
        <tr>
          <td>Accuracy</td>
          <td>High accuracy due to batch layer with complete data; speed layer may produce approximate results</td>
          <td>Consistent real-time results but may lack batch-layer level accuracy for complex computations</td>
        </tr>
        <tr>
          <td>Scalability</td>
          <td>Scales horizontally but more complex scaling due to separate batch and speed layers</td>
          <td>Easier to scale stream processing horizontally; simpler operational model</td>
        </tr>
        <tr>
          <td>Historical Data Handling</td>
          <td>Excellent, supports deep historical batch analytics and corrections</td>
          <td>Less suited for complex historical data analysis, designed mainly for streaming real-time data</td>
        </tr>
        <tr>
          <td>Implementation Complexity</td>
          <td>High development and maintenance effort due to dual pipelines and serving layer integration</td>
          <td>Lower implementation and maintenance overhead</td>
        </tr>
        <tr>
          <td>Consistency Between Layers</td>
          <td>Requires careful coordination to keep batch and speed outputs consistent</td>
          <td>Single pipeline avoids consistency issues inherent in Lambda dual-layer design</td>
        </tr>
        <tr>
          <td>Real-Time Analytics</td>
          <td>Provides real-time insights via speed layer but with possible eventual consistency lag</td>
          <td>Provides immediate real-time analytics with no separate batch delay</td>
        </tr>
        <tr>
          <td>Support for Complex Analytics</td>
          <td>Good support since batch layer handles heavy, complex queries and aggregations</td>
          <td>Limited complex analytics, as everything must be handled in stream processing</td>
        </tr>
        <tr>
          <td>Reprocessing Complexity</td>
          <td>Batch layer reprocessing is separate and managed independently</td>
          <td>Reprocessing simply involves re-consuming the event stream, simplifying error correction</td>
        </tr>
        <tr>
          <td>Data Duplication Risk</td>
          <td>Potential for duplication or mismatch between batch and speed layer results if not carefully managed</td>
          <td>Minimal duplication risk since there is only one data processing pipeline</td>
        </tr>
        <tr>
          <td>Use Cases</td>
          <td>Suitable for systems needing both comprehensive historical analysis and real-time insights</td>
          <td>Best for real-time focused applications with simpler operational needs (e.g., IoT, user activity tracking)</td>
        </tr>
        <tr>
          <td>Examples</td>
          <td>Recommendation engines, financial modeling, large-scale analytics</td>
          <td>Real-time monitoring, IoT analytics, clickstream processing, social media analytics</td>
        </tr>
      </tbody>
    </table>

  </TabItem>
  <TabItem value="testing" label="Testing">
    <table className="text_vertical">
      <thead>
        <tr>
          <th>Type</th>
          <th>Purpose</th>
          <th>Scope</th>
          <th>When Performed</th>
          <th>Key Techniques</th>
          <th>Considerations</th>
          <th>Relevance</th>
          <th>Quality Checks</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Data Quality Testing</td>
          <td>Validate accuracy, completeness, consistency, validity, timeliness, and uniqueness of data</td>
          <td>Data at rest (tables, datasets) and in-motion (streams)</td>
          <td>Often ongoing, triggered by data load or refresh</td>
          <td>Profiling, validation rules, anomaly detection, null checks, deduplication</td>
          <td>Identifying subtle quality issues, evolving data schemas</td>
          <td>Crucial for trustworthy analytics; foundation to all downstream processes</td>
          <td>
            <ul>
              <li><b>Descriptive Checks:</b> Validating data entries represent real-world values (e.g., valid email format, phone numbers)</li>
              <li><b>Structural Checks:</b> Ensuring data conforms to schema (all required fields present, data types correct)</li>
              <li><b>Integrity Checks:</b> Validating relationships between datasets (e.g., foreign keys match primary keys)</li>
              <li><b>Accuracy Checks:</b> Comparing data against trusted source systems</li>
              <li><b>Timeliness Checks:</b> Confirming data freshness and updates within defined periods</li>
              <li><b>Null or Missing Values:</b> Identifying nulls where data is mandatory</li>
              <li><b>Duplicate Data:</b> Detecting duplicated records that could cause inconsistencies</li>
              <li><b>Range and Distribution Checks:</b> Confirming numeric data falls within expected ranges or distributions</li>
              <li><b>Consistency Checks:</b> Ensuring data consistency across systems and datasets</li>
              <li><b>Format Validation:</b> Checking that data values meet predefined formats or patterns</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td>Data Integrity Testing</td>
          <td>Ensure accuracy, completeness, retrievable, verifiable, truthfulness, consistency, and reliability of data throughout its lifecycle</td>
          <td>Data storage, processing, transmission, and updates</td>
          <td>Routine and triggered by data changes, migrations</td>
          <td>Validation rules, checksums, version control, continuous monitoring, domain and entity integrity tests</td>
          <td>Managing volume & complexity, real-time validation, compliance, security</td>
          <td>Critical to maintain trustworthiness of data; prevents corruption and errors across all data states and systems</td>
          <td>
            <ul>
              <li><b>Accuracy:</b> Data matches real-world truth</li>
              <li><b>Reliability:</b> Repeatable results in different conditions</li>
              <li><b>Completeness:</b> No missing data required to maintain integrity</li>
              <li><b>Referential Integrity:</b> Relationships between tables/systems hold true (e.g., foreign keys)</li>
              <li><b>Repeatability:</b> Consistent test outcomes over time</li>
              <li><b>Scalability:</b> Tests effective under large datasets</li>
              <li><b>Validation Against Requirements:</b> Checking adherence to data constraints, ranges, and allowed values</li>
              <li>Automated anomaly detection to spot unusual data patterns</li>
              <li>Testing in isolated, production-like environments to avoid disruption</li>
              <li>Monitoring error resolution and anomaly frequency metrics</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td>Integration Testing</td>
          <td>Verify interactions and data flow between integrated components or systems</td>
          <td>Endpoints, APIs, data sources, ETL components</td>
          <td>After component/unit testing, pre-system integration</td>
          <td>API calls, contract validation, mock testing</td>
          <td>Managing dependencies, environment setup, flaky tests</td>
          <td>Ensure data flows cleanly between systems without loss or corruption</td>
          <td>
            <ul>
              <li>Testing interactions between microservices, databases, and APIs</li>
              <li>Verifying data formats and response correctness during data exchanges</li>
              <li>Using mocks/stubs to simulate unavailable services</li>
              <li>Automated API testing with tools like REST-assured or Postman</li>
              <li>Continuous integration (CI) pipeline integration to run tests upon changes</li>
              <li>Validation of data transformations during integration</li>
              <li>Monitoring logs and failures with detailed reporting</li>
              <li>Ensuring correct error handling in data communication</li>
              <li>Approaches include top-down, bottom-up, and big-bang integration testing</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td>Performance Testing</td>
          <td>Assess system responsiveness, throughput, stability under load</td>
          <td>Entire pipeline throughput, resource usage, latency</td>
          <td>Pre-release or after significant changes</td>
          <td>Load testing, stress testing, volume testing</td>
          <td>Simulating realistic load, environment parity</td>
          <td>Essential to meet SLAs for batch and streaming jobs, avoid bottlenecks</td>
          <td>
            <ul>
              <li><b>Load Testing:</b> Measuring system behavior under expected data volumes</li>
              <li><b>Stress Testing:</b> Testing beyond normal capacity limits</li>
              <li><b>Soak Testing:</b> Running systems under load over extended time to find memory leaks</li>
              <li><b>Spike Testing:</b> Sudden large surges of data volume</li>
              <li>Measuring response times, throughput, latency, and resource utilization</li>
              <li>Validating batch and streaming pipeline processing times</li>
              <li>Ensuring system remains responsive with increasing data sizes</li>
              <li>Starting performance tests early in development to catch issues quickly</li>
              <li>Time frames vary: from minutes for load/stress/spike tests, hours for soak tests</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td>Regression Testing</td>
          <td>Ensure new code/changes do not break existing data workflows or features</td>
          <td>Entire data pipeline or specific modules</td>
          <td>After any change or update</td>
          <td>Automated retesting, test case prioritization</td>
          <td>Test suite maintenance, execution time</td>
          <td>Maintain pipeline stability; detect silent errors after changes</td>
          <td>
            <ul>
              <li>Re-running previously passed tests on updated data pipelines/systems</li>
              <li>Validating functional and non-functional features remain stable</li>
              <li>Automated re-execution of test scripts upon schema or code changes</li>
              <li>Checking data accuracy, completeness, and transformations remain correct</li>
              <li>Detecting "immutable changes" where data changes should not occur</li>
              <li>Maintaining a regression test suite for quick verification with new code deploys</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td>End-to-End Testing</td>
          <td>Validate complete workflows from ingestion through transformations to consumption</td>
          <td>Across all pipeline stages and downstream applications</td>
          <td>Before major releases or deployment</td>
          <td>Full process simulation, real user scenario emulation</td>
          <td>High complexity, environment parity</td>
          <td>Confirm entire data lifecycle works as expected from source to consumer</td>
          <td>
            <ul>
              <li>Verifying data ingestion, processing, storage, and output in one flow</li>
              <li>Testing from data source event to final display or report generation</li>
              <li>Ensuring downstream integrations (notification, payments, reports) work</li>
              <li>Covering functional and non-functional aspects like usability and security</li>
              <li>Performing both automated and manual E2E tests on realistic data</li>
              <li>Monitoring for broken workflows or data errors affecting user journeys</li>
              <li>Employing tools like Cypress, Selenium, Playwright for automation</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td>Functional Testing</td>
          <td>Validates specific functions or business rules within data transformations</td>
          <td>Specific ETL jobs, SQL functions, or data logic blocks</td>
          <td>During development and after changes</td>
          <td>Unit tests, SQL assertions, black-box testing</td>
          <td>Test data setup, mock dependencies</td>
          <td>Validate correctness of data transformations and business logic</td>
          <td>
            <ul>
              <li>Testing data processing logic against defined specifications</li>
              <li>Validating outputs based on expected input data</li>
              <li>Checking edge cases and error handling paths</li>
              <li>Verifying correctness of data transformations</li>
              <li>Focus on "what" the system does, not "how" internally</li>
              <li>Manual and automated tests to validate individual features</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td>Compliance Testing</td>
          <td>Verify data adherence to legal, regulatory, and internal policies</td>
          <td>Data privacy, retention, access controls, audit trails</td>
          <td>Scheduled or triggered by regulation changes</td>
          <td>Policy validation, audit log review</td>
          <td>Dynamic rules, audits, cross-system consistency</td>
          <td>Ensure data governance and regulatory compliance requirements are met</td>
          <td>
            <ul>
              <li>Validating data privacy laws adherence (e.g., GDPR, HIPAA)</li>
              <li>Checking data encryption is applied where required</li>
              <li>Ensuring retention and deletion policies are enforced</li>
              <li>Auditing data access controls and audit trails</li>
              <li>Verifying reporting meets regulatory requirements</li>
              <li>Penetration testing and security compliance checks often integrated</li>
              <li>Documenting compliance evidence and configurations</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td>Contract Testing</td>
          <td>Verify that communication contracts/interfaces between services remain consistent</td>
          <td>API schemas, data contracts, message formats</td>
          <td>Before and during integration releases</td>
          <td>Schema validation, consumer-driven contract testing</td>
          <td>Coordinating consumer/provider contracts</td>
          <td>Prevent integration breakage due to incompatible schema changes</td>
          <td>
            <ul>
              <li>Checking APIs adhere to agreed contracts (request and response formats)</li>
              <li>Verification of data types, mandatory fields, and error codes</li>
              <li>Ensuring backward compatibility of APIs</li>
              <li>Using consumer-driven contract testing frameworks</li>
              <li>Automated tests executed in CI pipelines</li>
              <li>Preventing integration failures due to contract violations</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td>Data Processes Testing</td>
          <td>Validate ETL/ELT logic, correctness of data transformation and processing</td>
          <td>Extract, Transform, Load stages individually and combined</td>
          <td>During development, scheduled after pipeline changes</td>
          <td>Unit tests, integration tests, system-wide data validation</td>
          <td>Complex dependencies, state handling</td>
          <td>Ensure processing steps handle data correctly and produce expected results</td>
          <td>
            <ul>
              <li>Validating ETL/ELT logic correctness</li>
              <li>Ensuring data filtering, mapping, aggregation perform as expected</li>
              <li>Checking handling of nulls, missing data</li>
              <li>Verifying business rules implementations</li>
              <li>Testing intermediate outputs for correctness</li>
              <li>Automating process-level unit tests and scenario tests</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td>Pipeline Testing</td>
          <td>End-to-end and targeted tests validating pipeline orchestration, error handling, and data flow</td>
          <td>Orchestrator workflows, triggers, retries, alerts</td>
          <td>Continuous, after pipeline deployments or fixes</td>
          <td>Workflow simulations, failure scenario testing</td>
          <td>Environment parity, handling intermittent failures</td>
          <td>Verify pipeline robustness, alerting, and data delivery completeness</td>
          <td>
            <ul>
              <li>Testing pipeline orchestration logic and scheduling</li>
              <li>Validation of data flow correctness through all stages</li>
              <li>End-to-end latency and throughput monitoring</li>
              <li>Fault tolerance and error recovery testing</li>
              <li>Automated tests triggered by pipeline runs</li>
              <li>Integration with monitoring/alerting systems</li>
              <li>Versioning and rollback testing for pipelines</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
  </TabItem>
  <TabItem value="infrastructure-as-code" label="Infrastructure as Code">
    <Tabs queryString="secondary">
      <TabItem value="imperative-declarative" label="Imperative/Declarative" attributes={{className:"tabs__vertical"}}>
        Infrastructure as Code (IaC) provisions and manages computing infrastructure using code instead of manual processes. It reduces time-consuming errors, especially at scale, by defining desired states and automating deployment. This frees developers to focus on applications, while organizations gain cost control, risk reduction, and faster responses to opportunities.

        <table>
          <thead>
            <tr>
              <th>Aspect</th>
              <th>Imperative Programming</th>
              <th>Declarative Programming</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><b>Definition</b></td>
              <td>Specifies <em>how</em> to perform tasks step-by-step through explicit instructions</td>
              <td>Specifies <em>what</em> the desired outcome or goal is, without detailing how to achieve it</td>
            </tr>
            <tr>
              <td><b>Programming Approach</b></td>
              <td>The developer writes detailed instructions explicitly controlling each step to change the program state</td>
              <td>Describes the desired end state; the system figures out the instructions to reach that state automatically</td>
            </tr>
            <tr>
              <td><b>Control Flow</b></td>
              <td>Explicit; the developer manages the exact order of operations and flow</td>
              <td>Implicit; controlled by the system or runtime</td>
            </tr>
            <tr>
              <td><b>State Management</b></td>
              <td>Explicit and manual; the developer must maintain and update system state</td>
              <td>Abstracted away and handled automatically by the system</td>
            </tr>
            <tr>
              <td><b>Level of Abstraction</b></td>
              <td>Lower-level, deals with detailed procedural steps and direct system operations</td>
              <td>Higher-level, more abstract, focuses on logic and outcomes</td>
            </tr>
            <tr>
              <td><b>Error Handling</b></td>
              <td>Must be explicitly handled by the programmer; easier to introduce state inconsistency or errors</td>
              <td>Often more robust due to abstraction; the system validates state before applying changes</td>
            </tr>
            <tr>
              <td><b>Flexibility/Control</b></td>
              <td>More control over performance and optimization by managing each operation exactly</td>
              <td>Less fine-grained control over execution details, focus is on describing end results</td>
            </tr>
            <tr>
              <td><b>Maintainability</b></td>
              <td>Can become complex and harder to maintain with scaling due to detailed step management</td>
              <td>Typically easier to maintain and extend as logic is expressed declaratively</td>
            </tr>
            <tr>
              <td><b>Adaptability to State</b></td>
              <td>Rigid; instructions may fail if the initial state differs from assumptions</td>
              <td>Adaptive; compares current state with desired state and adjusts actions dynamically</td>
            </tr>
            <tr>
              <td><b>Performance</b></td>
              <td>Potentially faster for low-level tasks when optimized by expert programmers</td>
              <td>May add overhead from abstraction or compilation; optimized by underlying engine</td>
            </tr>
            <tr>
              <td><b>Error-Prone</b></td>
              <td>More prone to errors due to manual state & control flow management</td>
              <td>Generally less error-prone since system manages steps and state consistency</td>
            </tr>
            <tr>
              <td><b>Debugging</b></td>
              <td>Easier for step-by-step tracing but can get complicated in large codebases</td>
              <td>Debugging declarative code may be harder due to abstraction, requires understanding system internals</td>
            </tr>
            <tr>
              <td><b>Tools</b></td>
              <td>`Chef` and `Puppet`</td>
              <td>`Terraform`</td>
            </tr>
            <tr>
              <td><b>Use Cases</b></td>
              <td>Writing detailed data processing pipelines, manual orchestration of ETL steps, data cleaning scripts</td>
              <td>Defining database schemas, data transformations (`dbt` models), infrastructure as code (`Terraform`), SQL queries</td>
            </tr>
            <tr>
              <td><b>Example: Creating Table (SQL)</b></td>
              <td>Write explicit commands to create table, add columns, alter structure; may fail if structure exists</td>
              <td>Define the desired table structure and let the system handle creation or alteration dynamically</td>
            </tr>
            <tr>
              <td><b>Example Analogy</b></td>
              <td>Giving step-by-step instructions on how to make the sandwich starting from scratch</td>
              <td>Showing a picture of the final sandwich and having a competent chef make it</td>
            </tr>
          </tbody>
        </table>
      </TabItem>
      <TabItem value="idempotency" label="Idempotency">
        Idempotency means an operation can be applied multiple times without changing the result beyond the initial application.

        ### Importance

        - Prevents duplicate data processing and corruption during retries
        - Simplifies error handling by making retries safe
        - Ensures consistent and deterministic pipeline outputs
        - Enables scalable, concurrent processing without complex locking
        - Facilitates easier debugging and auditing
        - Meets strict regulatory compliance for transactional data

        ### Guidelines

        - **Use Idempotency Keys**:
          - Assign unique identifiers to each operation or data item
          - Use composite keys (e.g., source + timestamp) to detect duplicates
          - Store these keys to recognize repeated operation attempts and avoid reprocessing
        - **Employ Atomic Transactions**:
          - Group operations into atomic units that either complete fully or rollback entirely
          - Use transactional ACID-compliant storage systems where possible
        - **Deduplication Techniques**:
          - Implement deduplication at multiple levels (data ingestion, processing, storage)
          - Utilize probabilistic data structures (Bloom filters) and sliding window algorithms for efficient duplicate detection
        - **Checkpointing and State Management**:
          - Maintain and persist checkpoints/states for recovery and partial processing resumption
          - Enable pipeline to restart safely from the last consistent state after failures
        - **Use Contextual Uniqueness**:
          - Incorporate business logic attributes in idempotency checks to catch logical duplicates
        - **Concurrency Control**:
          - Design systems that handle concurrent writes gracefully using idempotency
          - Leverage modern concurrency control patterns like non-blocking concurrency
        - **Choose Idempotent Storage Backends**:
          - Leverage storage systems that support conditional updates or compare-and-swap semantics (e.g., Delta Lake, Apache Hudi, distributed NoSQL with ACID features)

        ### Testing and Validation

        ### Validation Techniques

        - **Testing Methodologies**
          - **Repeated Execution Testing**: Re-run operations multiple times and verify the same state
          - **Fault Injection Testing**: Simulate failures (network, crashes) to observe idempotent behavior
          - **Concurrent Operation Testing**: Run identical operations simultaneously to test race conditions
          - **State Transition Validation**: Confirm system transitions remain consistent regardless of operation frequency
          - **Time-Window Testing**: Retry operations across time spans to ensure idempotency holds over time
        - **Validation Techniques**
          - **Range Checking**: Validate data values fall within acceptable limits
          - **Type Checking**: Verify data types conform to expectations
          - **Format Checking**: Ensure compliance with required data formats (e.g., emails, phone numbers)
          - **Consistency Checks**: Confirm relational integrity across fields and datasets
        - **Automated Testing**
          - Property-based testing to generate varied and edge-case scenarios
          - Chaos engineering tools to introduce faults in production-like environments
          - Integration and regression tests to maintain idempotency guarantees as systems evolve
          - Performance monitoring to assess idempotency overhead
      </TabItem>
    </Tabs>

  </TabItem>
  <TabItem value="security" label="Security">
    <Tabs queryString="secondary">
      <TabItem value="security-overview" label="Overview" attributes={{className: 'tabs__vertical'}}>
        <table>
          <thead>
            <tr>
              <th>Aspect</th>
              <th>Authentication</th>
              <th>Authorization</th>
              <th>Encryption</th>
              <th>Tokenization</th>
              <th>Data Masking</th>
              <th>Data Obfuscation</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Definition</td>
              <td>Verifying identity of a user or system</td>
              <td>Granting or denying access rights to resources</td>
              <td>Transforming data into unreadable format to protect it</td>
              <td>Replacing sensitive data with non-sensitive tokens</td>
              <td>Replacing sensitive data with fictitious but realistic data</td>
              <td>Hiding data through transformation to prevent understanding</td>
            </tr>
            <tr>
              <td>Purpose</td>
              <td>Confirming who is accessing the system</td>
              <td>Controlling what authenticated users can do/access</td>
              <td>Protecting data confidentiality during storage/transit</td>
              <td>Safeguarding sensitive data by replacing it with tokens</td>
              <td>Protecting sensitive info while keeping data useful</td>
              <td>Preventing data exposure while often preserving format</td>
            </tr>
            <tr>
              <td>Scope</td>
              <td>Identity level (user, device, service)</td>
              <td>Permission level (file, operation, service)</td>
              <td>Data at rest, in transit</td>
              <td>Specific sensitive data fields/elements</td>
              <td>Databases, tables, fields, datasets for testing/sharing</td>
              <td>Various data forms, often to resist reverse engineering</td>
            </tr>
            <tr>
              <td>Reversibility</td>
              <td>N/A (identity verification)</td>
              <td>N/A (access control)</td>
              <td>Reversible if decryption key is held</td>
              <td>Usually reversible via token vault, some are irreversible</td>
              <td>Usually irreversible; aim is to prevent data recovery</td>
              <td>Usually irreversible or complex to reverse</td>
            </tr>
            <tr>
              <td>Security Focus</td>
              <td>Identity assurance</td>
              <td>Access control enforcement</td>
              <td>Confidentiality, data leakage prevention</td>
              <td>Strong data security with minimal data exposure</td>
              <td>Privacy compliance, risk reduction</td>
              <td>Anti-reverse engineering, protecting intellectual property</td>
            </tr>
            <tr>
              <td>Data Format Preservation</td>
              <td>N/A</td>
              <td>N/A</td>
              <td>Does not preserve original data format visibly</td>
              <td>Can preserve format (format-preserving tokenization)</td>
              <td>Preserves data usability and format</td>
              <td>Often preserves structure/format for usability</td>
            </tr>
            <tr>
              <td>Performance Impact</td>
              <td>Low to medium, depends on method</td>
              <td>Low to medium, depends on complexity of policies</td>
              <td>Can be high, especially with strong encryption and large data</td>
              <td>Medium, due to token vault and lookups</td>
              <td>Low to medium, depending on masking method (static/dynamic)</td>
              <td>Low to medium, depends on obfuscation technique</td>
            </tr>
            <tr>
              <td>Complexity</td>
              <td>Can be complex (multi-factor, adaptive)</td>
              <td>Can be complex with fine-grained policies and delegation</td>
              <td>Complex key management and cryptographic implementation</td>
              <td>Complex token vault/database management</td>
              <td>Intermediate; requires design of masking policies</td>
              <td>Intermediate; requires custom transformation/logics</td>
            </tr>
            <tr>
              <td>Regulatory Compliance</td>
              <td>Supports compliance by preventing unauthorized access</td>
              <td>Supports compliance by enforcing access control</td>
              <td>Strong support for data privacy and protection laws</td>
              <td>Helps meet PCI DSS, GDPR by masking real data</td>
              <td>Ensures compliance with GDPR, HIPAA, CCPA in testing/sharing</td>
              <td>Assists compliance by protecting sensitive info exposure</td>
            </tr>
            <tr>
              <td>Key Limitation</td>
              <td>Doesn't control resource access beyond identity verification</td>
              <td>Authz policies can be bypassed if authN is weak</td>
              <td>Key management critical; if keys lost, data unrecoverable</td>
              <td>Reliance on token vault security; complexity</td>
              <td>May reduce realism or break referential integrity</td>
              <td>Can be reverse-engineered if weak transformations used</td>
            </tr>
            <tr>
              <td>Use Cases</td>
              <td>Logins, multi-factor auth, biometric verification</td>
              <td>Role-based access control, attribute-based access control</td>
              <td>Securing emails, files, network traffic, databases</td>
              <td>Payment card processing, PII protection, API token usage</td>
              <td>Test/dev environments, analytics with safe data, compliance</td>
              <td>Protecting source code, data export, secure telemetry</td>
            </tr>
            <tr>
              <td>Example Techniques</td>
              <td>Passwords, biometrics, OTP, SSO</td>
              <td>RBAC, ABAC, ACLs, policy engines</td>
              <td>AES, RSA, TLS/SSL, hashing</td>
              <td>Format-preserving tokenization, stateless/stateful tokens</td>
              <td>Substitution, shuffling, scrambling, nulling, encryption-based masking</td>
              <td>Character substitution, ciphering, noise addition</td>
            </tr>
          </tbody>
        </table>
      </TabItem>
      <TabItem value="authentication" label="Authentication">

        ## Evolution of Authentication Methods

        ```mermaid
            graph TB

            subgraph auth [WWW-Authentication]
              direction LR

              authUser(User) e1@--> |username + password| authServer(Server)
            end

            auth e2@--> |inability to control the login lifecycle| session

            subgraph session [Session-Cookie]
              direction LR

              sessionUser(User) e3@--> |cookie| sessionServer(Server)
              sessionServer e4@--> sessionDb[(DB)]
              sessionDb e5@--> |session ID| sessionUser
            end

            session e6@--> |no mobile support| token

            subgraph token [Token-Based]
              direction LR

              tokenUser(User) e7@--> |token| tokenServer(Server)
              tokenServer e8@--> |validate token| tokenValidator(Token Validation Service)
            end

            token e9@--> |reduce token validation| jwt

            subgraph jwt [JWT]
              direction LR

              jwtToken(Token: header.payload.signature)
            end

            jwt e10@--> |cross-site login| sso

            subgraph sso [SSO]
              direction LR

              ssoUser(User) e11@--> ssoDomain1(a.com)
              ssoUser e12@--> ssoDomain2(b.com)
              ssoUser e13@--> ssoDomain3(c.com<br/>CAS - Central Authentication Service)
              ssoDomain1 e14@--> ssoDomain3
              ssoDomain2 e15@--> ssoDomain3
            end

            sso e16@--> |3rd party access| oauth

            subgraph oauth [OAuth 2.0]
                direction LR

                oauthUser(OAuth 2.0) e17@--> |browser & server| code(Authentication Code)
                oauthUser e18@--> |server only| credentials(User Credentials)
                oauthUser e19@--> |implicit grant| native(Native App)
            end

            e1@{ animate: true }
            e2@{ animate: true }
            e3@{ animate: true }
            e4@{ animate: true }
            e5@{ animate: true }
            e6@{ animate: true }
            e7@{ animate: true }
            e8@{ animate: true }
            e9@{ animate: true }
            e10@{ animate: true }
            e11@{ animate: true }
            e12@{ animate: true }
            e13@{ animate: true }
            e14@{ animate: true }
            e15@{ animate: true }
            e16@{ animate: true }
            e17@{ animate: true }
            e18@{ animate: true }
            e19@{ animate: true }
        ```

        ## Credentials (Base64)

        ```mermaid
          sequenceDiagram
          autonumber

          participant Client
          box Server
            participant Server
            participant Database
          end

          note left of Client: Form with username and password

          Client->>Server: HTTPS Connection <br/> Credentials Encryption with SSL
          Server->>Server: Decrypts with SSL cert private key
          Server->>Database: Username lookup & hashed password verification
          Database->>Server: User record with hashed password
          Server->>Client: Authentication Status
        ```

        ## JSON Web Token (JWT)

        <table>
            <thead>
                <tr>
                    <th style={{width: '70%'}}>Visualization</th>
                    <th>Specs</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>
                        ```mermaid
                            sequenceDiagram
                            autonumber

                            participant User
                            participant Server

                            box Signing Algorithm
                                participant JWT Provider
                                participant JWT Consumer
                            end

                            User->>Server: Login <br/> username & password
                            Server->>Server: Validate credentials
                            Server->>JWT Provider: Create & Sign JWT with secret

                            alt Public Key
                                JWT Provider->>JWT Provider: Sign JWT with private key
                                JWT Provider->>JWT Consumer: Signed JWT + Public Key
                                JWT Consumer->>JWT Consumer: Verify with public key (RS256, ES256)
                            end

                            alt Symmetric Key
                                JWT Provider->>JWT Provider: Sign JWT with public key
                                JWT Provider->>JWT Consumer: Signed JWT
                                JWT Provider-->JWT Consumer: Shared public key
                                JWT Consumer->>JWT Consumer: Verify with public key (HS256, HMAC)
                            end

                            JWT Provider->>Server: Signed JWT
                            Server->>User: Authorization Bearer JWT Token
                            User->>User: Store JWT locally
                            User->>Server: /resource/book<br/>Authorization Bearer JWT Token
                            Server->>Server: Validate signature
                            Server->>User: OK - access granted
                        ```
                    </td>
                    <td>
                        <ul>
                            <li>
                                <b>JWT Structure</b>
                                <ul>
                                    <li>
                                    Content
                                        <ul>
                                            <li>
                                            Header

                                            ```json
                                            {
                                                "alg": "HS256",
                                                "type": "JWT"
                                            }
                                            ```
                                            </li>
                                            <li>
                                                Data

                                                ```json
                                                {
                                                    "user_id": 1234,
                                                }
                                                ```
                                            </li>
                                            <li>Signature: `HMACSHA256("base64(header).base64(data)", secret)`</li>
                                        </ul>
                                    </li>
                                    <li>Encode each part using Base64: `base64(header)`, `base64(data)`, `base64(signature)`</li>
                                    <li>Concatenate each part using dot (`.`): `base64(header).base64(data).base64(signature)`</li>
                                </ul>
                            </li>
                        </ul>
                    </td>
                </tr>
            </tbody>
        </table>

        ## Oauth 2.0

        <table>
          <thead>
            <tr>
              <th style={{width: '70%'}}>Visualization</th>
              <th>Specs</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>
                ```mermaid
                  sequenceDiagram
                  autonumber

                  participant User
                  participant Web App
                  participant Auth0 Tenant (Identity Provider)
                  participant Server

                  User->>Web App: Click login link
                  Web App->>Auth0 Tenant (Identity Provider): Authorization Code Request to `/authorize`
                  Auth0 Tenant (Identity Provider)->>Web App: Redirect to `login/authorization` prompt
                  User->>Auth0 Tenant (Identity Provider): Authenticate & Consent
                  Auth0 Tenant (Identity Provider)->>Web App: Authorization Code
                  Web App->>Auth0 Tenant (Identity Provider): Authorization Code for Application Credentials
                  Auth0 Tenant (Identity Provider)->>Auth0 Tenant (Identity Provider): Validate Authorization Code & Application Credentials
                  Auth0 Tenant (Identity Provider)->>Web App: ID Token & Access Token
                  Web App->>Server: Request user data with Access Token
                  Server->>Web App: Response
                ```
              </td>
              <td>
                <ul>
                    <li>
                        <b>Open Authorization (OAuth)</b>: Protocol for sharing user Authorization across systems
                        <ul>
                            <li><b>OAuth 1.0</b>: Protocol designed only for web browser only</li>
                            <li><b>OAuth 2.0</b>: Protocol for cross-platform use (web, mobile, desktop, API)</li>
                        </ul>
                    </li>
                    <li>
                        <b>Involved Parties</b>
                        <ul>
                          <li><b>User (Resource Owner)</b>: Authorizes flows across systems</li>
                          <li><b>Identity Provider (IdP)</b>: Stores user identity, validates credentials, and shares authorization with other services</li>
                          <li><b>Server</b>: Service user accesses for authorization</li>
                        </ul>
                    </li>
                    <li>
                        <b>Flow Types</b>
                        <ul>
                          <li><b>Authorization code</b>: Client gets a code from server, exchanges for access token</li>
                          <li><b>Client credentials</b>: Client directly authenticates for access to its resources</li>
                          <li><b>Implicit code</b>: Deprecated due to security risks</li>
                          <li><b>Resource owner password</b>: User's credentials exchanged for access token, not recommended for security reasons</li>
                        </ul>
                    </li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        ## SSH Keys

        ```mermaid
          sequenceDiagram
          autonumber

          participant Client
          participant Server

          note left of Client: Host key: public/private userkey
          Client->>Server: Authenticator userkey <br/> public key
          note right of Server: Public userkey & client host keys

          note right of Server: Host key: public/private userkey
          Server->>Client: Authenticator hostkey <br/> server authentication
          note left of Client: Public host keys
        ```

        ## SSL Certificates

        ```mermaid
          sequenceDiagram
          autonumber

          participant Client
          participant Server

          Client->>Server: https://google.com
          Server->>Client: SSL Certificate

          Client->>Client: Validity Expiry Check
          Client->>Client: Issue Authority (CA) Check
          Client->>Client: Domain Name Match Check

          Client->>Server: Random Encrypted Key
          Server->>Server: Decrypts with SSL cert private key

          Server->>Client: Secured Connection
        ```

        ## 2FA (Two-Factor Authentication)

        <table>
            <thead>
                <tr>
                    <th style={{width: '80%'}}>Visualization</th>
                    <th>Specs</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>
                        ```mermaid
                          sequenceDiagram
                          autonumber

                          participant User
                          participant Authentication Service
                          participant Database
                          participant Authenticator Client

                          alt Stage 1: User enables 2SA Service
                            User->>Authentication Service: Request a secret key
                            Authentication Service->>Authentication Service: Generate secret key
                            Authentication Service->>Database: Store secret key
                            Authentication Service->>User: URI in a form of a QR code<br/>otpauth://topt/issuer:user?secret=secretkey<br/>⇣<br/>otpauth://topt/google:Joe?secret=E884S34
                            User->>Authenticator Client: Scan QR code
                            User->>User: Store secret key in Authenticator
                          end

                          alt Stage 2: User uses 2SA Service for authentication
                          Authenticator Client->>Authenticator Client: every 30 seconds refreshes secret key (6-digit number)<br/>using TOPT (Time-based One-Time Password) algorithm<br/>⇣<br/>secret key + timestamp = 6-digit number
                          Authenticator Client->>User: Enter generated password
                          User->>Authentication Service: Send password to server
                          Authentication Service->>User: Comparison result of client-side password and server-side password
                          Authentication Service->>Database: Read secret key<br/>generates password using the same TOTP algorithm<br/>as authenticator client<br/>⇣<br/>secret key + timestamp = 6-digit number
                          end
                        ```
                    </td>
                    <td>
                        <ul>
                            <li>
                                <b>Safety</b>
                                <ul>
                                    <li>Secret key transmission via `HTTPS`</li>
                                    <li>Encryption of secret keys in client and database</li>
                                </ul>
                            </li>
                            <li>
                                <b>Security</b>
                                <ul>
                                    <li>Password (6-digit number) has 1 million combinations</li>
                                    <li>Changes every 30 seconds, making it hard to guess</li>
                                </ul>
                            </li>
                            <li>
                                <b>2FA Code Types</b>
                                <ul>
                                    <li>SMS code, scratch card, mobile app</li>
                                    <li>Hardware token: U2F FIDO key, MFA token, digital ID</li>
                                    <li>Biometric system: finger/hand print, iris scan, behavior/movement tracking</li>
                                </ul>
                            </li>
                        </ul>
                    </td>
                </tr>
            </tbody>
        </table>

        ## 2SA (Two-Step Authentication)

        ```mermaid
              sequenceDiagram
              autonumber

              participant User
              participant Server
              participant Access Granted

              User->>Server: username,password &<br/>2SV (Step Verification) token/cookie

              alt Validate username & password
                Server->>User: no - auth failed
                Server->>Server: yes
              end

              alt 2SV Valid
                Server->>Access Granted: yes
                Server->>Server: no
              end

              Server->>Server: Prompt for OTP (One-Time Password)
              alt OTP
                note over Server: Single Factor, 2SV<br/>SMS, email
                note over Server: Two Factor, 2SV<br/>Google Auth, SmartCard
              end

              alt 2SV code valid
                Server->>User: no - password compromised
                Server->>Server: yes
              end

              alt User trust this device
                Server->>User: yes - store 2SV cookie
                Server->>Access Granted: no
              end
        ```
      </TabItem>
      <TabItem value="authorization" label="Authorization">
        <table>
          <thead>
            <tr>
              <th>Aspect</th>
              <th>Role-Based Access Control (RBAC)</th>
              <th>Attribute-Based Access Control (ABAC)</th>
              <th>Access Control List (ACL)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Concept</td>
              <td>Assigns permissions to users based on their roles within an organization</td>
              <td>Grants access based on attributes of the user, resource, environment, and context</td>
              <td>A list of specific rules defining who can access an object and what actions allowed</td>
            </tr>
            <tr>
              <td>Main Focus</td>
              <td>Roles and their associated permissions</td>
              <td>Attributes and policies combining them</td>
              <td>Explicit rules tied to individual resources</td>
            </tr>
            <tr>
              <td>Key Components</td>
              <td>Users, Roles, Permissions, Sessions</td>
              <td>Subjects (users), Objects (resources), Actions, Environment, Policies</td>
              <td>Resources, Access Control Entries (ACEs) specifying users/groups and their permissions</td>
            </tr>
            <tr>
              <td>Visualization</td>
              <td>
                ```mermaid
                flowchart TD
                    subgraph User Authorization Request
                        A(User Requests Access) e1@--> B(User Identity Verified)
                        B e2@--> C(Retrieve User Roles)
                    end

                    subgraph RBAC System
                        C e3@--> D(Check Roles Assigned to User)
                        D e4@--> E(Retrieve Permissions for Roles)
                        E e5@--> F{Action Permitted}
                    end

                    subgraph Resource Access
                        F e6@-->|Grant| G(Grant Access to Resource)
                        F e7@-->|Deny| H(Deny Access)
                    end

                    e1@{ animate: true }
                    e2@{ animate: true }
                    e3@{ animate: true }
                    e4@{ animate: true }
                    e5@{ animate: true }
                    e6@{ animate: true }
                    e7@{ animate: true }
                ```
              </td>
              <td>
                ```mermaid
                flowchart TD
                    subgraph User Request
                        A(User Requests Access) e1@--> B(User Identity Verified)
                        B e2@--> C(Attributes Collected from Subject, Object, Action, Environment)
                    end

                    subgraph Policy Evaluation
                        C e3@--> D("Send Attributes to Policy Decision Point (PDP)")
                        D e4@--> E(Evaluate Policies Using Attributes)
                        E e5@--> F{Policy Decision}
                    end

                    subgraph Enforcement
                        F e6@-->|Grant| G("Policy Enforcement Point (PEP) Grants Access")
                        F e7@-->|Deny| H(PEP Denies Access)
                    end

                    e1@{ animate: true }
                    e2@{ animate: true }
                    e3@{ animate: true }
                    e4@{ animate: true }
                    e5@{ animate: true }
                    e6@{ animate: true }
                    e7@{ animate: true }
                ```
              </td>
              <td>
                ```mermaid
                flowchart TD
                    subgraph User Request
                        A(User Requests Access to Resource) e1@--> B(Verify User Identity)
                    end

                    subgraph Access Control List Check
                        B e2@--> C(Retrieve ACL for Requested Resource)
                        C e3@--> D(Check if User is in ACL)
                        D e4@--> E{Access Allowed}
                    end

                    subgraph Enforcement
                        E e5@-->|Grant| F(Grant Access to Resource)
                        E e6@-->|Deny| G(Deny Access)
                    end

                    e1@{ animate: true }
                    e2@{ animate: true }
                    e3@{ animate: true }
                    e4@{ animate: true }
                    e5@{ animate: true }
                    e6@{ animate: true }
                ```
              </td>
            </tr>
            <tr>
              <td>Access Control Model</td>
              <td>Role-centric, static binding of permissions</td>
              <td>Policy-centric, dynamic evaluation of attributes at request time</td>
              <td>Rule-centric, access defined by explicit rules for users or groups per resource</td>
            </tr>
            <tr>
              <td>Flexibility</td>
              <td>Moderate. Roles predefined; less adaptable to context changes</td>
              <td>High. Can consider dynamic and contextual information (time, location, device, etc.)</td>
              <td>Low to moderate. Rules usually static and manually maintained</td>
            </tr>
            <tr>
              <td>Granularity</td>
              <td>Coarse to moderate, depends on number and granularity of roles</td>
              <td>Fine-grained; policies can combine multiple attributes for precise decisions</td>
              <td>Fine-grained at resource level, specifying detailed permissions per user/object</td>
            </tr>
            <tr>
              <td>Scalability</td>
              <td>Scales well with a manageable number of roles; risk of role explosion if too many roles created</td>
              <td>Can become complex and computationally heavy with many attributes and policies</td>
              <td>Can be complex to manage at scale if many resources and users require rules</td>
            </tr>
            <tr>
              <td>Administration</td>
              <td>Centralized administration through role assignments; easier for compliance audits</td>
              <td>Complex policy administration requiring careful attribute and policy design</td>
              <td>Decentralized - resource owners or admins define ACLs; can be cumbersome</td>
            </tr>
            <tr>
              <td>Policy Evaluation</td>
              <td>At user-login or session creation, roles assigned then used throughout session</td>
              <td>Real-time evaluation of attributes at each access request</td>
              <td>Each access request evaluated against ordered ACL rules sequentially</td>
            </tr>
            <tr>
              <td>Security Strength</td>
              <td>Good for static deterministic control but vulnerable if roles have excessive privileges</td>
              <td>Potentially stronger due to fine-grained, context-aware policies</td>
              <td>Strong when rules are well managed; can be prone to errors if rules overlap</td>
            </tr>
            <tr>
              <td>Policy Complexity</td>
              <td>Simpler conceptually and easier to implement for basic needs</td>
              <td>More complex, requiring detailed attribute and policy management</td>
              <td>Simple for small sets of resources but can become complex</td>
            </tr>
            <tr>
              <td>Typical Policy Components</td>
              <td>Roles, permissions, users, sessions</td>
              <td>Attributes (user, resource, environment), policies, rules combining attributes</td>
              <td>Access Control Entries (ACEs) specifying users/groups and their permissions</td>
            </tr>
            <tr>
              <td>Errors and Conflicts</td>
              <td>Role explosion can create overlap or excessive permissions</td>
              <td>Policy conflicts can be complex to detect and resolve</td>
              <td>Rule ordering is critical; earlier rules take precedence, leading to conflicts if mismanaged</td>
            </tr>
            <tr>
              <td>Management Overhead</td>
              <td>Moderate; fewer roles means simpler management but can grow with complexity of roles</td>
              <td>Higher due to attribute and policy complexity</td>
              <td>High if many resources/users require individualized ACLs</td>
            </tr>
            <tr>
              <td>User Control</td>
              <td>No direct control by end users; all managed by administrators</td>
              <td>No direct user control; policy-driven access</td>
              <td>Owners may control ACLs on their resources (discretionary control)</td>
            </tr>
            <tr>
              <td>Compliance and Auditing</td>
              <td>Easier to audit due to defined roles and permissions</td>
              <td>More complex auditing due to dynamic policies but more precise logging possible</td>
              <td>Auditable if ACLs are properly logged and maintained</td>
            </tr>
            <tr>
              <td>Hybrid Use</td>
              <td>Often combined with ABAC for context-aware refinements</td>
              <td>Can include role as an attribute or integrate with RBAC</td>
              <td>ACLs often used alongside RBAC or ABAC for network or low-level access control layers</td>
            </tr>
            <tr>
              <td>Example Permissions</td>
              <td>"HR Manager" role can approve leave requests and view payroll data</td>
              <td>User accessing resource only during business hours and from corporate device</td>
              <td>IP-based allow/deny rules on network devices or file read/write permissions per user</td>
            </tr>
            <tr>
              <td>Use Cases</td>
              <td>Enterprises with clearly defined job functions and structured hierarchies</td>
              <td>Environments needing fine-grained, dynamic, context-aware access decisions</td>
              <td>Network devices (routers, firewalls), file systems, and simple resource-based control</td>
            </tr>
            <tr>
              <td>Common Implementations</td>
              <td>Microsoft Active Directory, Oracle RBAC, databases, enterprise IT systems</td>
              <td>Healthcare, finance, government systems with strict compliance needs</td>
              <td>Router/firewall rules, Windows/Linux file system permissions, some databases</td>
            </tr>
          </tbody>
        </table>
      </TabItem>
    </Tabs>

  </TabItem>
  <TabItem value="data-mesh" label="Data Mesh">
    <Tabs queryString="secondary">
      <TabItem value="overview" label="Overview" attributes={{ className: 'tabs__vertical' }}>
        Data mesh is a decentralized data architecture where teams own and manage their data. It assigns ownership to business domains (e.g., finance, marketing, sales), providing a self-serve platform and federated governance. This enables autonomous development of tailored data services while ensuring a unified data experience across the organization.

        <table>
          <thead>
            <tr>
              <th>Aspect</th>
              <th>Domain Ownership</th>
              <th>Data as a Product</th>
              <th>Self-Serve Data Platform</th>
              <th>Federated Governance</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Strategic Domain Driven Design</td>
              <td>Domain Bounded Context</td>
              <td>Product Thinking</td>
              <td>Domain-Agnostic</td>
              <td>Context-Mapping</td>
            </tr>
            <tr>
              <td>Socio-technical Perspective</td>
              <td>Domain Teams</td>
              <td>Data Product by Domain Team</td>
              <td>Data Platform Team</td>
              <td>Guild</td>
            </tr>
            <tr>
              <td>Technology</td>
              <td>Operational & Analytical Data</td>
              <td>Interoperability Interfaces</td>
              <td>Self-Serve Data Platform</td>
              <td>Data Governance & Automation</td>
            </tr>
          </tbody>
        </table>

        ## Core Principles

        - **Domain-oriented** decentralized ownership: Business domains (e.g., customer service, marketing) own and manage their analytical and operational data services, tailoring data models to their needs
        - **Data as a product**: domain teams treat other domains as consumers, providing high-quality, secure, and up-to-date data
        - **Self-service data infrastructure as a platform**: dedicated team provides tools for domains to autonomously consume, develop, deploy, and manage interoperable data products
        - **Federated computational governance**: centralized governance authority with embedded governance in each domain's processes, enabling autonomy while ensuring compliance

        ## Data Mesh Architecture

        ```mermaid
        flowchart LR
          subgraph FederatedGovernnce [Federated Governance]
            direction TB
            interoperability(Interoperability Policy)
            documentation(Documentation Policy)
            security(Security Policy)
            privacy(Privacy Policy)
            compliance(Compliance Policy)
          end

          governanceGroup[[Governance Group]] e1@-->|supports| FederatedGovernnce

          subgraph selfServeDataPlatform [Self-Serve Data Platform]
            direction TB
            storageAndQueryEngine(Storage & Query Engine)
            dataProductCatalog(Data Product Catalog)
            dataContractManagement(Data Contract Management)
            monitoring(Monitoring)
            policyAutomation(Policy Automation)
          end

          dataPlatformTeam[[Data Platform Team]] e2@-->|supports| selfServeDataPlatform

          subgraph training [Training]
            direction TB
            consulting(Consulting)
            examples(Examples)
            bestPractices(Best Practices)
          end

          enablingTeam1[[Enabling Team]] e3@-->|supports| training

          subgraph domain1
          end

          subgraph domain2
            direction LR
            dataContract2(Data Contract)
            dataProduct2(Data Product)
            analytics2(Analytics)
            operationalData2(Operational Data)

            analytics2 e4@-->|analyze| dataProduct2
            operationalData2 e5@-->|ingest| dataProduct2
            dataProduct2 e6@-->|publish| dataContract2
          end

          domain1 e7@-->|use| dataContract2

          domainTeam2[[Domain Team 2]] e8@---->|supports| domain2
          dataProduct2 e9@--->|use| dataProduct3

          subgraph domain3
            direction TB
            dataProduct3(Data Product)
          end

          e1@{ animate: true }
          e2@{ animate: true }
          e3@{ animate: true }
          e4@{ animate: true }
          e5@{ animate: true }
          e6@{ animate: true }
          e7@{ animate: true }
          e8@{ animate: true }
          e9@{ animate: true }
        ```

        ## Data Product

        ```mermaid
        flowchart TB
          subgraph DataProduct [Data Product]
            direction TB
            ownership(Ownership & Lifecycle)
            transformation(Transformation Code)
            tests(Tests)
            documentation(Documentation)
            dataStorage[(Data Storage)]
            costManagement(Cost Management)
            policies(Policies as Code)
            cicd(CI/CD Pipeline)
            observability(Observability)
          end

          discoveryPort(Discovery Port<br/>Metadata) e1@--> DataProduct

          inputPortOps(Input Port<br/>Operational Systems) e2@--> DataProduct
          inputPortData(Input Port<br/>Other Data Products over Data Contract) e3@--> DataProduct

          DataProduct e4@--> outputPort1(Output Port<br/>Data Model & Technology)
          DataProduct e5@--> outputPort2(Output Port<br/>Data Model & Technology)

          e1@{ animate: true }
          e2@{ animate: true }
          e3@{ animate: true }
          e4@{ animate: true }
          e5@{ animate: true }
        ```

        ## High-level Platform Design and Governance

        ![](./assets/data-mesh/high-level-platform-design.svg)

        ## Example

        ![](./assets/data-mesh/example.svg)
      </TabItem>
      <TabItem value="governance-topologies" label="Governance Topologies">
        <table className="text_vertical">
          <thead>
            <tr>
              <th>Aspect</th>
              <th>Fine-grained Fully Federated Mesh</th>
              <th>Fine-grained Fully and Fully Governed Mesh</th>
              <th>Hybrid Federated Mesh</th>
              <th>Value Chain-Aligned Mesh</th>
              <th>Coarse-grained Aligned Mesh</th>
              <th>Coarse-grained and Governed Mesh</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><b>Description</b></td>
              <td>Pure data mesh model with many small, independent deployable components, peer-to-peer data distribution, logically centralized governance metadata</td>
              <td>Adds a central data distribution layer to fine-grained federated mesh for stronger governance and centralized data distribution</td>
              <td>Combines federation and centralization. Central platform hosts/maintains data products; domain autonomy mainly in data consumption</td>
              <td>Domains aligned along business value chains, working in close groups with autonomy but sharing central standards for cross-domain data</td>
              <td>Large, coarse-grained domains, often as a result of mergers; domains contain many applications, organic growth leads to complexity</td>
              <td>Similar to coarse-grained aligned mesh but with stronger governance features like addressing time-variant and non-volatile data concerns</td>
            </tr>
            <tr>
              <td><b>Visualization</b></td>
              <td>![](./assets/data-mesh/fine-grained-fully-federated-mesh.svg)</td>
              <td>![](./assets/data-mesh/fine-grained-fully-and-fully-governed-mesh.svg)</td>
              <td>![](./assets/data-mesh/hybrid-federated-mesh.svg)</td>
              <td>![](./assets/data-mesh/value-chain-aligned-mesh.svg)</td>
              <td>![](./assets/data-mesh/coarse-grained-aligned-mesh.svg)</td>
              <td>![](./assets/data-mesh/coarse-grained-and-governed-mesh.svg)</td>
            </tr>
            <tr>
              <td><b>Granularity</b></td>
              <td>Fine-grained data products, many small independent units</td>
              <td>Fine-grained data products with centralized distribution layer</td>
              <td>Hybrid: fine to moderate granularity; central platform more involved</td>
              <td>Fine to moderate granularity aligned by value chains</td>
              <td>Coarse-grained domains containing many applications</td>
              <td>Coarse-grained domains with governed attributes</td>
            </tr>
            <tr>
              <td><b>Governance Approach</b></td>
              <td>Federated with logically centralized metadata governance but mostly domain autonomy</td>
              <td>Fully governed with central control over distribution and conformance</td>
              <td>Governed but with domain autonomy in consumption; central platform manages creation/maintenance</td>
              <td>Central standards for cross-domain data; requires architectural guidance</td>
              <td>Strong governance policies necessary due to complexity</td>
              <td>Fully governed with relaxed controls in large domains</td>
            </tr>
            <tr>
              <td><b>Data Distribution</b></td>
              <td>Peer-to-peer between domains; domains share data directly</td>
              <td>Centralized data distribution via shared storage layer (domain-specific containers)</td>
              <td>Domains create/manage data via central platform; consumes data autonomously</td>
              <td>Aligned along value chains; domains share as needed with governance</td>
              <td>Centralized/shared to manage complexity across coarse domains</td>
              <td>Centralized/shared with governance controls for data quality</td>
            </tr>
            <tr>
              <td><b>Ownership</b></td>
              <td>Domain owns, manages, shares data independently</td>
              <td>Clear boundaries with domain ownership but central distribution</td>
              <td>Domain teams or platform team may own/manage data products depending on capability</td>
              <td>Domains collaborate with autonomy within their value chain</td>
              <td>Domain ownership but domains large and complex</td>
              <td>Domains own data but comply with governance for consistency</td>
            </tr>
            <tr>
              <td><b>Complexity / Management</b></td>
              <td>High complexity managing many small data products; needs conformance agreement across domains</td>
              <td>Higher complexity with governance and central controls; may slow time-to-market</td>
              <td>Moderate complexity; need supporting platform and governance team to manage hybrid roles</td>
              <td>Requires architectural coordination to define boundaries and standards clearly</td>
              <td>High complexity due to coarse domains and multiple applications</td>
              <td>High complexity with additional governance overhead</td>
            </tr>
            <tr>
              <td><b>Scalability</b></td>
              <td>Scales well horizontally but can be costly and resource-intensive due to duplication</td>
              <td>Scales with strong conformance but may have coupling delays and cost overheads centralized</td>
              <td>Scales with centralized platform efficiency and local domain agility</td>
              <td>Scales by value chains enabling domain group specialization</td>
              <td>Suited to large enterprises with many legacy systems and apps</td>
              <td>Similar to coarse-grained aligned but with governance improves scale consistency</td>
            </tr>
            <tr>
              <td><b>Network / Infrastructure Impact</b></td>
              <td>Potential for heavy network utilization and infrastructure duplication</td>
              <td>More efficient central infrastructure with shared storage and compute pools</td>
              <td>Some reduction in duplication with central platform; moderate overhead</td>
              <td>Balanced infrastructure demands due to group alignment</td>
              <td>Infrastructure complexity due to large domain size and app count</td>
              <td>Higher infrastructure cost but managed for compliance and quality</td>
            </tr>
            <tr>
              <td><b>Challenges & Risks</b></td>
              <td>Requires consensus on standards; potential data gravity vs decentralization conflict; costly infrastructure</td>
              <td>Longer time to market, potential domain coupling; challenge in multi-cloud seamless governance</td>
              <td>Management overhead with mixed governance; complex rules for data distribution</td>
              <td>Need strong architectural guidance; boundaries may be fluid and require attention</td>
              <td>Data alignment issues with domain boundaries; capability duplication</td>
              <td>Balancing autonomy with strong governance may slow flexibility</td>
            </tr>
            <tr>
              <td><b>Governed Data Characteristics</b></td>
              <td>Metadata governance centralized, data governance mostly at domain level</td>
              <td>Stronger data quality, compliance, and governance enforced centrally</td>
              <td>Governance mixed: central for product creation, federated for consumption</td>
              <td>Governance focuses on cross-domain data product standards</td>
              <td>Governance policies critical due to scale and complexity</td>
              <td>Governance addresses time-variant, compliance, and quality controls</td>
            </tr>
            <tr>
              <td><b>Use Cases</b></td>
              <td>Cloud-native, multi-cloud companies with many skilled engineers and high autonomy</td>
              <td>Financial institutions, governments valuing compliance over agility</td>
              <td>Organizations with legacy systems or lacking fully skilled teams; partial mesh</td>
              <td>Organizations needing stream-alignment or hyper-specialized domain cooperation (e.g., supply chain)</td>
              <td>Large enterprises with complex merged systems & applications</td>
              <td>Large enterprises needing governance and compliance in complex domains</td>
            </tr>
          </tbody>
        </table>
      </TabItem>
    </Tabs>

  </TabItem>
</Tabs>
