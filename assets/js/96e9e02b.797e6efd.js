"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[551],{11914:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/16-resource-aware-optimization-34046d045f3c234e11668b94d26e9bc1.svg"},11998:(e,n,t)=>{t.d(n,{A:()=>r});t(59729);var s=t(13526);const a={tabItem:"tabItem_JZCZ"};var i=t(65813);function r({children:e,hidden:n,className:t}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,s.A)(a.tabItem,t),hidden:n,children:e})}},12920:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/13-human-in-the-loop-c797e8c32bb5cb1510a7cd378462e894.svg"},16549:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/10-mcp-9b6e9f15ac3d09b44102ef7f6ab124ab.svg"},17602:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/15-a2a-7b56e2777d3f6a0ed8b11eb2933d79b5.svg"},20508:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(59729);const a={},i=s.createContext(a);function r(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:n},e.children)}},23687:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/02-router-a46ef12527d75266b477cb91a3360d2f.svg"},25181:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/08-memory-management-dcc95307f9d5967af234514f440ea868.svg"},32246:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/03-parallelization-94efe89eebcabdf7ab1537937e170365.svg"},33953:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/11-goal-setting-and-monitoring-835c922078e97f11b8532705a74a264f.svg"},34204:(e,n,t)=>{t.d(n,{A:()=>y});var s=t(59729),a=t(13526),i=t(7267),r=t(25470),o=t(96177),c=t(63188),l=t(22301),d=t(47714);function u(e){return s.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function g(e){const{values:n,children:t}=e;return(0,s.useMemo)((()=>{const e=n??function(e){return u(e).map((({props:{value:e,label:n,attributes:t,default:s}})=>({value:e,label:n,attributes:t,default:s})))}(t);return function(e){const n=(0,l.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m({value:e,tabValues:n}){return n.some((n=>n.value===e))}function h({queryString:e=!1,groupId:n}){const t=(0,r.W6)(),a=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(a),(0,s.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(t.location.search);n.set(a,e),t.replace({...t.location,search:n.toString()})}),[a,t])]}function p(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,i=g(e),[r,c]=(0,s.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find((e=>e.default))??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:i}))),[l,u]=h({queryString:t,groupId:a}),[p,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,a]=(0,d.Dv)(n);return[t,(0,s.useCallback)((e=>{n&&a.set(e)}),[n,a])]}({groupId:a}),b=(()=>{const e=l??p;return m({value:e,tabValues:i})?e:null})();(0,o.A)((()=>{b&&c(b)}),[b]);return{selectedValue:r,selectValue:(0,s.useCallback)((e=>{if(!m({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);c(e),u(e),f(e)}),[u,f,i]),tabValues:i}}var f=t(83891);const b={tabList:"tabList_Mt30",tabItem:"tabItem_dm4R"};var x=t(65813);function v({className:e,block:n,selectedValue:t,selectValue:s,tabValues:r}){const o=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.a_)(),l=e=>{const n=e.currentTarget,a=o.indexOf(n),i=r[a].value;i!==t&&(c(n),s(i))},d=e=>{let n=null;switch(e.key){case"Enter":l(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:r.map((({value:e,label:n,attributes:s})=>(0,x.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:l,...s,className:(0,a.A)("tabs__item",b.tabItem,s?.className,{"tabs__item--active":t===e}),children:n??e},e)))})}function j({lazy:e,children:n,selectedValue:t}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find((e=>e.props.value===t));return e?(0,s.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==t})))})}function A(e){const n=p(e);return(0,x.jsxs)("div",{className:(0,a.A)("tabs-container",b.tabList),children:[(0,x.jsx)(v,{...n,...e}),(0,x.jsx)(j,{...n,...e})]})}function y(e){const n=(0,f.A)();return(0,x.jsx)(A,{...e,children:u(e.children)},String(n))}},35189:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/19-evaluation-and-monitoring-4b71b8d55376e092444acd48038ca9c1.svg"},37355:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/21-exploration-and-discovery-77f4a1f32f9854ccf8380c3ff1df8c13.svg"},38782:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/17-reasoning-techniques-58c5726ca085956336ab78a25dbf1a9e.svg"},41597:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/20-prioritization-67fbf1eead7b0e3ca84fdcc639c75a29.svg"},46833:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/04-reflection-3b31dcae1d7126516c24383c3a82449c.svg"},49275:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/14-rag-e21f1e1080f4f850510bf8cfb84742a2.svg"},57622:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/12-exception-handling-and-recovery-6c8260969a878c4303fc077b6be30d7c.svg"},63095:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/06-planning-f1485e7bbf275cffdca53aa7b996502b.svg"},65038:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>m,frontMatter:()=>c,metadata:()=>s,toc:()=>u});const s=JSON.parse('{"id":"education/AI Engineer/agentic","title":"Agentic","description":"Agentic AI Overview","source":"@site/docs/education/10-AI Engineer/03-agentic.mdx","sourceDirName":"education/10-AI Engineer","slug":"/education/AI Engineer/agentic","permalink":"/ml-labs/docs/education/AI Engineer/agentic","draft":false,"unlisted":false,"editUrl":"https://github.com/EliFuzz/ml-labs/docs/education/10-AI Engineer/03-agentic.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Agentic","description":"Agentic AI Overview","hide_table_of_contents":true},"sidebar":"education","previous":{"title":"Fundamentals","permalink":"/ml-labs/docs/education/AI Engineer/fundamentals"},"next":{"title":"RAG","permalink":"/ml-labs/docs/education/AI Engineer/rag"}}');var a=t(65813),i=t(20508),r=t(11998),o=t(34204);const c={title:"Agentic",description:"Agentic AI Overview",hide_table_of_contents:!0},l=void 0,d={},u=[];function g(e){const n={img:"img",...(0,i.R)(),...e.components};return(0,a.jsx)(o.A,{queryString:"primary",children:(0,a.jsx)(r.A,{value:"design-patterns",label:"Design Patterns",children:(0,a.jsxs)("table",{className:"text_vertical",children:[(0,a.jsx)("thead",{children:(0,a.jsxs)("tr",{children:[(0,a.jsx)("th",{children:"Pattern"}),(0,a.jsx)("th",{style:{minWidth:"400px"},children:"Visualization"}),(0,a.jsx)("th",{children:"Definition"}),(0,a.jsx)("th",{children:"Use Cases"})]})}),(0,a.jsxs)("tbody",{children:[(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Prompt Chaining"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(87801).A+""})}),(0,a.jsx)("td",{children:"decomposes complex LLM tasks into sequential sub-problems, improving reliability by focusing each step and using structured outputs like JSON. It overcomes single-prompt limitations such as errors and hallucinations, enabling AI agents for multi-step reasoning and external tool integration"}),(0,a.jsx)("td",{children:"Information Processing Workflows; Complex Query Answering; Data Extraction and Transformation; Content Generation Workflows; Conversational Agents with State; Code Generation and Refinement; Multimodal and multi-step reasoning"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Routing"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(23687).A+""})}),(0,a.jsx)("td",{children:"enables dynamic decision-making in agentic systems, shifting from fixed sequential workflows to adaptive paths based on inputs like user intent or state. Mechanisms: LLM-based (prompt for classification), Embedding-based (semantic similarity), Rule-based (logic on keywords), ML Model-based (fine-tuned classifier)"}),(0,a.jsx)("td",{children:"Chain summarization; Trend analysis; Email drafting for market reports"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Parallelization"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(32246).A+""})}),(0,a.jsx)("td",{children:"enables concurrent execution of independent sub-tasks (e.g., LLM calls, tool usages, or sub-agents) in agentic workflows, unlike sequential Prompt Chaining or dynamic Routing. This reduces overall time by running tasks simultaneously, especially with external latencies"}),(0,a.jsx)("td",{children:"Information Gathering and Research;  Data Processing and Analysis; Multi-API or Tool Interaction; Content Generation with Multiple Components; Validation and Verification;  Multi-Modal Processing; A/B Testing or Multiple Options Generation"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Reflection"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(46833).A+""})}),(0,a.jsx)("td",{children:"enables AI agents to evaluate and refine their own outputs or processes, improving accuracy, completeness, and quality beyond basic patterns like Chaining, Routing, or Parallelization. It introduces a feedback loop where an agent critiques its work and iterates for better results"}),(0,a.jsx)("td",{children:"Creative Writing and Content Generation; Code Generation and Debugging; Complex Problem Solving; Summarization and Information Synthesis; Planning and Strategy; Conversational Agents"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Tool Use (Function Calling)"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(95045).A+""})}),(0,a.jsx)("td",{children:"enables agents to interact with external systems (APIs, databases, services, or code) via Function Calling. It allows LLMs to decide when and how to invoke tools based on user requests"}),(0,a.jsx)("td",{children:"Information Retrieval from External Sources; Interacting with Databases and APIs; Performing Calculations and Data Analysis; Sending Communications; Executing Code; Controlling Other Systems or Devices"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Planning"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(63095).A+""})}),(0,a.jsx)("td",{children:"enables an AI agent to autonomously create action sequences from an initial state to a goal state, adapting to new information and obstacles"}),(0,a.jsx)("td",{children:"Procedural Task Automation; Robotics and Autonomous Navigation; Structured Information Synthesis; Customer Support"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Multi-Agent Collaboration"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(94602).A+""})}),(0,a.jsx)("td",{children:"overcomes limitations of monolithic agents for complex, multi-domain tasks by decomposing objectives into sub-problems assigned to specialized agents (e.g., a Research Agent for retrieval, Data Analysis for processing, and Synthesis for reporting). Success relies on standardized communication protocols and shared ontologies for data exchange, delegation, and coordination, ensuring coherent outputs. Models include: single agent, network, supervisor, supervisor as a tool, hierarchical, and custom"}),(0,a.jsx)("td",{children:"Coordinated Research Efforts; Collaborative Data Analysis; Joint Content Creation; Multi-Agent Planning and Strategy"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Memory Management"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(25181).A+""})}),(0,a.jsx)("td",{children:"agents need memory like humans: short-term (contextual, in LLM windows; limited, ephemeral) and long-term (persistent, in databases; semantic retrieval for integration)"}),(0,a.jsx)("td",{children:"Chatbots and Conversational AI; Task-Oriented Agents; Personalized Experiences; Learning and Improvement; Information Retrieval (RAG); Autonomous Systems"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Learning and Adaptation"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(72951).A+""})}),(0,a.jsx)("td",{children:"AI agents improve autonomously through various learning types including reinforcement learning with reward systems, supervised learning using labeled data, unsupervised learning for pattern discovery, few-shot/zero-shot adaptation, online real-time updates, and memory-based contextual learning to handle novel situations. Key algorithms like Proximal Policy Optimization (PPO) provide stable reinforcement learning for continuous actions, while Direct Preference Optimization (DPO) offers a simpler approach to align large language models directly with human preferences without requiring separate reward models"}),(0,a.jsx)("td",{children:"Personalized assistant agents; Trading bot agents; Application agents; Robotic and autonomous vehicle agents; Fraud detection agents"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Model Context Protocol"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(16549).A+""})}),(0,a.jsx)("td",{children:"open standard enabling LLMs to interact with external tools, data, and systems through a client-server architecture, acting as a universal adapter for dynamic discovery and interoperability. It exposes resources (static data), tools (executable actions), and prompts (interaction templates), promoting reusability and composability while wrapping legacy systems without rewrites. Unlike proprietary tool function calling, MCP supports broad, standardized connections, with considerations for security, error handling, and agent-friendly data formats. The interaction flow involves discovery, request formulation, server execution, and response updates to enable advanced agentic behavior"}),(0,a.jsx)("td",{children:"Database Integration; Generative Media Orchestration; External API Interaction; Reasoning-Based Information Extraction; Custom Tool Development; Standardized LLM-to-Application Communication; Complex Workflow Orchestration; IoT Device Control; Financial Services Automation"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Goal Setting and Monitoring"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(33953).A+""})}),(0,a.jsx)("td",{children:"AI agents need defined objectives and progress tracking to succeed. This pattern sets goals, monitors advancement, and verifies success"}),(0,a.jsx)("td",{children:"Customer Support Automation; Personalized Learning Systems; Project Management Assistants; Automated Trading Bots; Robotics and Autonomous Vehicles; Content Moderation"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Exception Handling and Recovery"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(57622).A+""})}),(0,a.jsx)("td",{children:"enables AI agents to detect and recover from errors, malfunctions, and challenges using strategies like logging, retries, fallbacks, degradation, notifications, rollback, diagnosis, self-correction, and escalation, promoting resilience in unpredictable environments"}),(0,a.jsx)("td",{children:"Customer Service Chatbots; Automated Financial Trading; Smart Home Automation; Data Processing Agents; Web Scraping Agents; Robotics and Manufacturing"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Human-in-the-Loop"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(12920).A+""})}),(0,a.jsx)("td",{children:"integrates human judgment with AI to ensure ethical, safe deployment in complex domains, augmenting human capabilities through oversight, intervention, feedback, and collaboration. It enables AI use in sensitive sectors but faces scalability, expertise, and privacy challenges"}),(0,a.jsx)("td",{children:"Content Moderation; Autonomous Driving; Financial Fraud Detection; Legal Document Review; Customer Support (Complex Queries); Data Labeling and Annotation; Generative AI Refinement; Autonomous Networks"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Knowledge Retrieval (RAG)"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(49275).A+""})}),(0,a.jsx)("td",{children:"integrates external knowledge bases into LLMs for accurate, up-to-date responses. It uses semantic search to retrieve relevant chunks, augments prompts, and generates grounded outputs, reducing hallucinations. Core elements: embeddings (text vectors), similarity (semantic closeness), chunking (document breakdown), vector databases (efficient embedding storage/query). Variants: Graph RAG (knowledge graphs for complex queries); Agentic RAG (reasoning agent for validation, reconciliation, multi-step synthesis). Challenges: fragmented info, retrieval quality, maintenance costs, adds latency/complexity"}),(0,a.jsx)("td",{children:"Enterprise Search and Q&A; Customer Support and Helpdesks; Personalized Content Recommendation; News and Current Events Summarization"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Inter-Agent Communication (A2A)"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(17602).A+""})}),(0,a.jsx)("td",{children:"A2A is an open protocol for AI agents from different frameworks to collaborate via tasks, messages, and artifacts over JSON-RPC 2.0. Key features include Agent Cards for discovery, interaction modes (synchronous, polling, streaming, push), and security measures like TLS. Supported by major companies, it enhances multi-agent AI efficiency and complements MCP"}),(0,a.jsx)("td",{children:"Multi-Framework Collaboration; Automated Workflow Orchestration; Dynamic Information Retrieval"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Resource-Aware Optimization"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(11914).A+""})}),(0,a.jsx)("td",{children:"allows agents to dynamically allocate computational, temporal, and financial resources to meet goals within budgets or maximize efficiency. Agents choose between accurate/costly models and fast/cheap ones, such as quick summaries for limited resources or detailed forecasts for ample time/budget. A fallback switches to affordable models if needed, ensuring continuity"}),(0,a.jsx)("td",{children:"Cost-Optimized LLM Usage; Latency-Sensitive Operations; Energy Efficiency; Fallback for service reliability; Data Usage Management; Adaptive Task Allocation"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Reasoning Techniques"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(38782).A+""})}),(0,a.jsx)("td",{children:'reasoning techniques for AI agents, including multi-step inferences, problem decomposition, and increased inference resources for better accuracy. Key methods include Chain-of-Thought (CoT) for step-by-step logic, Tree-of-Thought (ToT) for branching paths, Self-Correction for refinement, ReAct for tool-integrated actions, and collaborative frameworks like Chain of Debates (CoD) and Multi-Agent System Search (MASS). The Scaling Inference Law enables smaller models to outperform larger ones with extended "thinking time," as seen in applications like Deep Research, fostering transparent, autonomous agents for complex tasks'}),(0,a.jsx)("td",{children:"Complex Question Answering; Mathematical Problem Solving; Code Debugging and Generation; Strategic Planning; Medical Diagnosis; Legal Analysis"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Guardrails/Safety"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(68741).A+""})}),(0,a.jsx)("td",{children:" ensure AI agents avoid harmful outputs via input validation, output filtering, behavioral constraints, and oversight. Examples: CrewAI uses prompts and Pydantic models; Vertex AI employs validation callbacks. Reliable agents need modularity, observability, least privilege, and fault-tolerant patterns like checkpoints for robustness"}),(0,a.jsx)("td",{children:"Customer Service Chatbots; Content Generation Systems; Educational Tutors/Assistants; Legal Research Assistants; Recruitment and HR Tools; Social Media Content Moderation; Scientific Research Assistants"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Evaluation and Monitoring"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(35189).A+""})}),(0,a.jsx)("td",{children:'enables AI agents to continuously assess performance, track progress toward goals, and detect operational anomalies through metrics, feedback loops, and reporting systems, ensuring alignment with requirements in dynamic environments. It incorporates response accuracy, latency monitoring, token usage tracking, LLM-as-a-Judge evaluations, agent trajectories, testing via files and evalsets, multi-agent dynamics, and advanced frameworks like "contractors" with formalized pillars for reliable, production-ready operation'}),(0,a.jsx)("td",{children:"Performance Tracking in Live Systems; A/B Testing for Agent Improvements; Compliance and Safety Audits; Enterprise systems; Drift Detection; Anomaly Detection in Agent Behavior; Learning Progress Assessment"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Prioritization"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(41597).A+""})}),(0,a.jsx)("td",{children:"helps AI agents in complex settings rank tasks by urgency, importance, dependencies, and resources, focusing on critical actions for efficiency and goal alignment. It includes defining criteria, evaluating tasks, selecting actions via algorithms, and re-prioritizing dynamically, applicable at goal, sub-task, or action levels"}),(0,a.jsx)("td",{children:"Automated Customer Support; Cloud Computing; Autonomous Driving Systems; Financial Trading; Project Management; Cybersecurity; Personal Assistant AIs"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("b",{children:"Exploration and Discovery"})}),(0,a.jsx)("td",{children:(0,a.jsx)(n.img,{src:t(37355).A+""})}),(0,a.jsx)("td",{children:"enables AI agents to proactively explore novel information in complex environments, moving beyond reactive optimization"}),(0,a.jsx)("td",{children:"Scientific Research Automation; Game Playing and Strategy Generation; Market Research and Trend Spotting; Security Vulnerability Discovery; Creative Content Generation; Personalized Education and Training"})]})]})]})})})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(g,{...e})}):g(e)}},68741:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/18-guardrails-bee09136627cc8c11effc4c3a660aa18.svg"},72951:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/09-learning-and-adaptation-ff1803b2bc3f774c83fe995976c28045.svg"},87801:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/01-prompt-chaining-ae1e7c38edb8ba477ffc568b55f0595a.svg"},94602:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/07-multi-agent-collaboration-9f27fbf5abcc7f1d00531d45bbea584b.svg"},95045:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/05-tool-use-97c13aa75b2ff6c4b535c03544fe0cbf.svg"}}]);